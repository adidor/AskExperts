[
    {
        "text": "since late April myself and machine learning engineer Josh Stapleton have evaluated over a hundred and twenty thousand answers from GPT models to explore their limits in my original smart GPT video I showed that even popular TED Talks calling gpt4 stupid were not accurately testing what gpt4 could do and actually it could easily get such questions right little did we foresee that come the summer our tests with Gypsy 4 would be revealing a host of mistakes in an official globally used Benchmark uncovering concerns that even open Ai and Google don't appear to be aware of but by the end of the video I want to show how you can tangibly benefit from our experiments including in unexpected domains like medicine where to start well here's a super quick intro to those of you who haven't seen the original video smartgbt was a way of using the latest prompt engineering research to trigger better performance in a model like GP E4 getting the model to think a bit AKA use some tokens before giving a final answer was key another important element I talked about in that video was the power of getting the model to self-reflect an Insight I drew on from talks with the lead author of the famous reflection paper my manual experiments showed that using optimized prompts reflection and self-dialogue you could boost performance in almost any task and I demoed the Improvement on formal logic and college mathematics but there was a problem which is why you guys haven't heard about smartgbt in a while how could I systematically Benchmark gc4 using these methods when I'm just one guy well enter Josh Stapleton machine learning engineer extraordinaire without him it would have been impossible to build out such a fleshed out flexible code base with which we could systematize experiments and iterate rapidly but then we both quickly realized that there was another problem with benchmarking the original",
        "start": "00:00:00",
        "duration": 229.49899999999994,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "version of smartgbt on tens of thousands of official questions it would be hell to manually extract out the final answers within pages of reflection and resolving not to mention cost tens of thousands of dollars and trust me a month of YouTube advertising would not even cover the first hour of that run unfortunately and no we would never compromise by asking gpt4 to grade its own answers it would be unscientific and inaccurate the infamous Mit paper is enough evidence of that gpt4 didn't get 100 on an MIT degree and this paper was withdrawn so yes we had to lower the power level of smart GPT get rid of the reflection and resolving deliberately sacrificing some of its intelligence because we simply couldn't afford to unleash it fully and yet we still got a new albeit unofficial record of 88.4 on the mmlu that not only beats the 86.4 recorded by open AI it beats the projections for 2024 the metaculus recorded before TPT came out and yet we are both convinced that there are at least a dozen more ways performance can be further boosted using existing models yes that might mean gpt4 getting a result reserved for June of 2025. the thing is we have hit the limits of what a self-funding team of two can do before I briefly touch on what the mmlu is I am happy to say that all of our results and answers that's 2850 for the gp4 run and over 120 000 for GPT 3.5 are freely available to browse in a GitHub repository Linked In the description so what the hell is mmlu anyway well it is arguably the best known Benchmark of language model performance it stands for massive multitask language understanding massive because it has over 14 000 questions and multitask because it covers 57 different domains the idea behind it was truly fantastic and it is important enough to feature prominently on the first page of the gpt4 technical report in the past I have said that getting a hundred percent",
        "start": "00:01:54",
        "duration": 265.979,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "on this test would be a good sign of AGI others have talked about 95 I do think I have like a 50 chance like within the next 20 years or so there might be something will be my call in HEI or a transformative AI what do I mean by this well so maybe can measure it on benchmarks there's like this famous mmlu benchmarks like yeah there's something which like scores like 95 on this and the paper itself notes that an 89.8 performance represents human expert ability which as you can tell from the title we are achingly close to beating and as you'll see in a moment gpt4 with the full power of prompt engineering could likely get upwards of 90 to 92 right now and frankly whether it's gpd5 or Gemini that 95 threshold should easily be broken by next year not in 20 years if we didn't use the full power of smart GPT how did we get 88.4 and why does the title say 89 well let me show you the two facets of smart GPT that we did use the thing is the mmlu demands A Single Character answer A B C or D and that answer must be immediate now imagine taking a test and the very first thought you had ad had to be your final answer on a quick tangent this is believed to be a key reason for hallucinations this was a great paper on how language model hallucinations can snowball from the first few tokens as they say the language model first commits to an answer that's before outputting the explanation and this is a problem because Transformers cannot find the answer within one time step because of their limited reasoning abilities within that times that and why don't language models like gpt4 back down and change halfway through well they prioritize fluency and coherence at the expense of factuality but rushing an answer in your first token is particularly hobbling in questions like this requiring deeper thought or calculation it's fine for questions that need memorized knowledge but not for",
        "start": "00:04:07",
        "duration": 242.99999999999997,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "questions like these we went through all of these subjects in the mmlu and characterized around a third of them as requiring that kind of deeper thought and of course most ways that you use gpd4 will also require some thought but I'll touch more on practical uses at the end open source teams and groups like openai and Google all draw on the devset when testing a model notice the five questions each with a single character answer we were not of course the first team to realize that this underplays the abilities of the model the Minerva paper from Google said this the standard way of evaluating on the mmlu is to construct a five shot prompt out of the dev set so what they did instead like us was to use a prompt which has a Chain of Thought before outputting the final answer you can see some examples below essentially it allows the model to think a bit first and gives it a scratch Pad there were other theories though like the length and detail of the exemplars triggering different weights of the model this paper from two months ago used longer exemplars for the moral scenario subject within the mmlu with these five custom them exemplars plus self-consistency which I'll get to in a moment they saw accuracy go up to 80 from 68 but before even this paper came out Josh and I were sourcing and crafting bespoke exemplars or the 21 subjects we deemed would need the most working out but the other subjects we may do with the normal Dev examples openai and Google don't do this for their benchmarking underplaying the abilities of their model so why doesn't everyone do it you might ask my theory is that it's because you have to hand grade every answer rather than just lazily check for an exact character match through Auto grading essentially you're taking the time to listen which is the least that we can do I feel as we approach human level intelligences even though it still took weeks to make",
        "start": "00:06:09",
        "duration": 240.30200000000005,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "checking easier we taught GPT 3.5 and gpd4 through our exemplars to always end with a final answer in the same format lesson one therefore for everyone watching is don't make the first token the final answer for the question you ask lesson two comes from a paper on self-consistency which in a nutshell says that taking the highest probability answer sometimes called greedy decoding doesn't always reflect the best answer the model is capable of in other words don't take the model's first answer as its final answer take this example the highest single probability answer was this and that was incorrect for open AI it would now be over it's incorrect done but sometimes if you look at all the different answers that a model might give and then take the majority answer the final answer that came up the most often it can get it right interestingly in the Minerva paper they used 256 samples although only 16 for the mmlu openai even put a little footnote in their gpt4 technical paper admitting that they don't use that approach but Google does and yes this can significantly affect the final results look at the Boost going up to 40 sampling paths and it hasn't fully leveled off yet these aren't redos where you keep trying until you get it right this is letting the model explore its full probability distribution of outputs and taking the truly most probable Final Answer letting the model think not rushing it for our runs we limited ourselves to nine samples and took the majority vote but of course the results could have been dramatically better if we did 40 samples or indeed 256. now aside from these two hard-won lessons which I'm going to show how all of you can benefit from the other difference from the previous state-of-the-art 86.4 was that we did use the most current versions of each model so the models may have independently gotten better or worse in certain topics but I would say",
        "start": "00:08:09",
        "duration": 240.59900000000002,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "that our broad findings do run counter to any simple it's got Dumber narrative and if I had to guess behind the scenes open AI have implemented some fine tuning involving step-by-step Solutions as I see that phrase cropping up in answers where it didn't used to and that particular trick from the original smartgbt seems less effective than before I'm now going to ask Josh to talk about our state of the art score not only with gpt4 but also with GPT 3.5 but just before I do here is a hint of why the title talks about breaking a benchmark I'll show you how gpt4 itself encouraged us to question many of the answers in the mmlu starting with one strange answer and then like pulling on a thread leading to the discovery of at least 80 and likely hundreds of errors in the test enough to significantly affect final results by up to two percent and given that the differences in say the open source language model leaderboard come down to as little as 0.1 of a percent that's pretty big yes we've been in contact with some of the authors of the test over the past month to check our findings and I'll say more in a bit but first here is ml engineer Josh detailing how the magic happened Josh by the way is a precocious AI consultant working on a masters at Imperial College London hi everyone nice to meet you all my name is Josh Stapleton and let me show you the version of smart GPT we used the smart GPT framework is highly parallelized and can handle industry scale use cases we used a thread and a sync i o based approach to make simultaneous calls to the API at answer option answer and subject levels stacking parallelization upon parallelization this led to Crazy iteration speed boosts for example we were able to complete the final gbt4 run in under two hours generating single answer options in series would have taken weeks we did two large runs using smart GPT first with GPT 3.5 and then",
        "start": "00:10:09",
        "duration": 235.86099999999996,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "gpt4 the 3.5 run was on the entirety of mmlu for a total of 9 times 14042 questions 126 000 answers this was a mammoth effort to manually grade but the smart GPT innovation and hard work ended up boosting GPT 3.5's performance by a significant 3.7 percent from 70 to 73.7 percent the gbt4 Run using smart GPT also beat the open AI mmru Benchmark score substantially and this run actually resulted in the discovery of a number of problematic mmlu questions which Philip will talk about shortly the cost to run gpt4 on all mmru would have been too high for us to self-fund having already each invested four figure sums so we used a representative subset of 2850 questions from the total of 14042 of course fully weighted two official standards smart GPT is a model agnostic parametrized and highly flexible system that can be applied to disparate use cases we are already working on applications in a number of domains in both the public and private sectors the system is evolving and improving constantly under the hood as we continue to innovate while the current system can get state-of-the-art results with the ability to handle Enterprise scale data there are a number of known ways to improve it which we aim to implement in the near future from better and more numerous automatically sourced exemplars to llm driven prompt optimization to fine-tuning we are just getting started with smart GPT and we are uniquely positioned as a tiny team to integrate both our own ongoing improvements as well as promising discoveries in the field as they arise back to Philip thank you Josh I will be working with you well at least until you're snapped up by a giant AI org anyway back to Breaking the Benchmark here is the question that started it all off as you can see the question makes no sense the text says demand reduction and the answers are either one three four two three four one",
        "start": "00:12:07",
        "duration": 235.259,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "two three one two four what on Earth is that about now remember it was only human grading that enabled us to spot this I reckon most companies like openai rely on auto grading by exact match that would immediately toss out any answer like these as no all because an answer of A B C or D hasn't been given now I should say it was human grading that caught this and gpt4 itself here is poor GPT 3.5 bravely giving an answer to a question that doesn't make absolutely any sense at all I love how a couple of times it changed its mind and was like no no D not B what Then followed was weeks and weeks of me following up every quote-unquote mistake with the official Source the question came from when I found the original Source I realized what the problem was sometimes they just hadn't pasted all of these statements when you can see all four of these statements the answer options make a lot more sense now I know what some of you may be thinking maybe it's just business ethics that's just one subject and what it's a dozen questions what's the big deal well first of all business ethics only has 100 questions so 13 of them missing vital context completely undermines that entire subject and second of all it wasn't just business ethics and it wasn't just this same problem it wouldn't always be about missing statements check out these examples from high school chemistry and there's High School psychology Professional Psychology microeconomics professional law professional accounting and trust me it didn't stop there I was genuinely shocked there were innumerable factual errors and I would try to trace down the origin of each and see what the source said by the way the problem wasn't just with one source it was with quite a few of these sources let a random take one of these questions how many human polyoma viruses are known at present 101 10 or unknown this question comes from Oxford University press",
        "start": "00:14:05",
        "duration": 227.82099999999994,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "chapter 21 question two I researched the question myself and also checked what this multiple choice quiz said the answer was let's tick 10 which is answer C and then submit my answers to the quiz and let's see yes it's correct it's 10. by the way the actual answer seems to be 14 as of 2023 but that's fairly close what does the mmlu say it's says the answer is a 100 and it goes on and on like this some of the worst offenders are the virology and college chemistry sections just wrong answer after wrong answer after wrong answer here's another example this is what the mmlu says is the answer to this question B I track down the question to a fall of 2011 final exam in which the answer was B the mmlu had mixed up the order of the options and therefore picked B when that was drug users instead of men but there's one more slight problem research suggests that both of those answers are inaccurate and that happened multiple times where even the source was somewhat dodgy with its answers here is another page of mistakes of virology and another page for college chemistry and one example that will particularly shock AI researchers here we have a question for which the mmlu says the correct answer is a the original Source says that the answer is eight which isn't even an option but this question was in the devset and if you remember from earlier in the video that is the set of five questions they used to teach the model what kind of answer to give when they're benchmarking in the mmlu in other words all 100 results in college chemistry for every model benched on the mmlu is compromised for example a model that is particularly good at imitating reasoning will now be imitating an incorrect answer now you might be thinking surely now Philip and gpt4 are done with finding errors in the mmlu unfortunately not we carry on into new categories here is a question from econometrics where again the source was incorrect but we",
        "start": "00:15:59",
        "duration": 242.39999999999998,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "also have misspellings grammatical ambiguity and formatting ambiguity throughout the test I'm not going to go through all of these but any one of them could potentially confuse a model we already know that models are very sensitive to the inputs you give are there any more categories yes there are there are loads of Juicy examples here but I can't get to them all how about example of multi-question dependents for example this came up in the philosophy section according to singer compliance with his principle requires but of course it doesn't say which of his principles or this one singer's argument begins with the assumption that now if you look at the original question it gave the context of the arguments it talked about the arguments and principles from his book famine affluence and morality but in the mmlu there is no such context and again and again this comes up high school bio you can see the two examples here now there is one final category I want to talk about no clear answer I'm not going to categorically call this a mistake but what answer would you pick here this is in public relations when an attitude is communicated what does it become an opinion A belief a behavior a point of view I think that's pretty ambiguous those kind of questions were particularly prevalent in moral scenarios and public relations or how about this are cryptocurrencies expensive or cheap is there an easy answer to that and there was one question that I did about three hours of research for what is the biggest cause of death in children under five years old and there are multiple sources that give conflicting answers that type of question where it depends what source you ask was massively prevalent in the global facts category and then we get controversial questions like this in security studies how do biological differences affect the roles that men",
        "start": "00:18:00",
        "duration": 209.69899999999998,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "and women must perform for the state with the correct answer being gender roles are a social construct I feel GPT 4's answer is far more nuanced this question touches on complex and controversial topics and while there is evidence to support or a few elements within each of the provided statements none of them fully captures the nuanced relationship between biology gender roles society and state responsibilities it also picks up on that language must perform for the state now remember these are just the examples that I found on a subset of the full test extrapolated out that would suggest that hundreds of questions are ambiguous or erroneous now a one two or three percent inaccuracy rate didn't really matter when models were performing at around 25 or 30 percent that's close to random and that was GPT 3's original performance but now when we're talking about AGI or human expert level accuracy and models are being judged on tenths of a percent one two or three percent really makes a big difference that's why the title says 89 because if you just remove the clear mistakes that I found of the valid questions our final result was 89.0 that number is of course more tentative as there may have been mistakes that both the test and gpt4 made that missed our scrupulous checks so at the moment we have a disparate set of benchmarks of varying degrees of validity and yes I've already begun to research some of the other benchmarks hello swag also appears to suffer from the first character equals Final Answer issue and honestly reviewing some of the questions I am more than tempted to order gpt4's mistakes on that Benchmark next of course I've been following attempts released very recently to develop cutting-edge benchmarks like legal bench and side bench they both look very promising I have even personally taught some of the topics that appear in other benchmarks like Aqua rat and AGI eval",
        "start": "00:19:45",
        "duration": 230.82000000000005,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "and let's not even get into Helm and other Suites of benchmarks that accept different formats of answer making comparisons confusing for Outsiders and even insiders I feel that the case is now strong for an independent professional benchmarking organization to be asked to step in perhaps one of the major education companies like Pearson funded by the top AGI Labs inspired by the Fantastic Vision behind the mmlu they could design an incredibly broad range of subject tests and those could be rolled into a benchmark that stretched all the way to extreme difficulties I've got more ideas on that maybe for another video each question could be rigorously vetted to be unambiguous and the volume and diversity of questions would reduce the kind of overfitting that people are worrying about now with human eval and of course the answers could also be blind human graded all models including open source ones held to the exact same standard and given the best chance to shine we need to Benchmark these models to the best of their abilities find the ceiling of what they can do not the floor as Paul Cristiano formerly of open AI has recently stated a broad understanding of AI capabilities and how these capabilities are likely to change over time would significantly reduce risks and I believe that there should be additional practical components to these benchmarks like questions on managing equipment in a bio lab as Dario amade of anthropic has speculated will soon be highly relevant and I would love to see modules designed by departments within AGI Labs made to detect how close AI is getting to being able to automate its own creation just this morning from semi-analysis we learned that Google Gemini is set to draw on five times the compute of gpd4 making these kind of benchmarks more urgent than ever so when Gemini gbt5 and Claude 3 come out we deserve to know the absolute limit of",
        "start": "00:21:40",
        "duration": 239.64,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "their ability rather than have that be discovered in the wild as openai's gpt4 technical report warned about but as promised to all of those who've come this far you deserve to see an example of how you can use all of this in the here and now to improve performance whether you're a multinational or an individual who's just curious look at this official medical diagnosis question of the type that an NHS doctor here in the UK would have to think through look what happens if you give no exemplars no real time to think and just take the first answer obviously not many of you are going to be Medical but this is a plausible diagnosis just not the most likely one it is definitively the incorrect answer in the NHS textbook but maybe that's just the one-off you're thinking nope I tried eight times I think every time from memory it gave that answer SLE or lupus which is incorrect let's see that's the third time if you can read these if you like but pretty much every time it's saying SLE and here is number eight Yes again SLE according to a professional medical opinion that I sought out gpt4 has picked up on some of the details but not on others it didn't pick up on the nodular appearance of the rash or the fact that it's on the forehead now let's give it some exemplars the more the merrier as you can see they're not directly related to the question but they help the model or wait do they this answer is again SLE but don't forget self-consistency those exemplars do help but you need to give the model time next time with the exact same prompt I got the answer sarcoidosis which is correct then SLE then sarcoidosis and this time sarcoidosis but for the final example I'm going to draw on the full power of smart GPT I'm going to get gpt4 to reflect on its answers obviously we couldn't do that for the full mmlu but we can do it here I plugged in the two diagnoses both of those came from gpd4",
        "start": "00:23:40",
        "duration": 229.68100000000004,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    },
    {
        "text": "and then I asked it to reflect which of these explanations is the most convincing based on the evidence supplied and this time the full smart GPT kicked in answer one was sarcoidosis as was answer two and answer three and you get the picture answer four and finally answer five Isn't that cool we went from always getting it wrong with no exemplars and no self-consistency to always getting it right with exemplars self-consistency and self-reflection obviously I am not saying that you should rely on gpd4 for medical diagnoses but I am saying that you might be surprised about the diversity of domains to which such methods can be applied and no such methods won't guarantee success after all we didn't get a hundred percent but it does push models closer to their limit and we are constantly reading the latest papers to further push performance whether it's skeleton of thoughts tree of thoughts graph of thoughts or thought experiments man that is a lot of thoughts so let us know if you've enjoyed this Mammoth ongoing effort or learn something from it and as ever thanks for watching to the end and have a wonderful day",
        "start": "00:25:35",
        "duration": 136.95999999999998,
        "title": "SmartGPT: Major Benchmark Broken - 89.0% on MMLU + Exam's Many Errors"
    }
]