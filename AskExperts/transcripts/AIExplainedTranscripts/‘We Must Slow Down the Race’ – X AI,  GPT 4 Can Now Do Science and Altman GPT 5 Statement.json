[
    {
        "text": "there were several significant developments in the last few days linked to gbt4 and openai I could honestly have done a video on each of them but realized that it might be better to do a single video tracing a single article covering seven major points I'm gonna use this fascinating piece from the Ft which millions of people have now read to run you through what has happened including Sam Altman's Revelation on Gypsy 5. Elon musk's new AI company and gpt4 conducting science the author by the way is an investor in anthropic and a co-author of the state of AI annual report and he puts it like this a three-letter acronym doesn't capture the enormity of what AGI would represent so I will refer to it as what it is Godlike AI this would be a super intelligent computer that learns and develops autonomously that understands its environment without the need for supervision and that can transform the world around it and the author Ian Hogan says we are not there yet but the nature of the technology makes it exceptionally difficult to predict exactly when we will get there the article presents this as a diagram with the exponential curve going up towards AGI and a much less impressive curve on the progress on alignment which he describes as a lining AI systems with human values now I know what some of you may be thinking surely those at the top of openai disagree on this gap between capabilities and Alignment well first here is yarn Leica who is the alignment team lead at openai what does he think he wants everyone to be reminded that aligning smarter than human AI systems with human values is an open research problem which basically means it's unsolved but what about those at the very top of open AI like Sam Altman when he was drafting his recent statement on the path to AGI he sent it to Nate Suarez of the machine intelligence Research Institute for one of the paragraphs Nate wrote this I",
        "start": "00:00:00",
        "duration": 239.159,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "think think that if we do keep running ahead with the current capabilities to alignment Ratio or even a slightly better one we die after this Sam Altman actually adjusted the statement adding that said it's important that the ratio of safety progress to capability progress increases going back to the article the author makes the point that there are not that many people directly employed in this area of alignment across the core AGI labs and what happened to that pause the experiment letter that I did a video on well as Hogarth points out the letter itself became a controversy so many people in my comments wrote that the only reason certain people are signing this is the slow open AI down so that they can catch up and this cynicism unfortunately has some new evidence that it can cite with musk forming his new AI company called xai this was reported 48 hours ago in the Wall Street Journal but people have seen this coming for months now apparently the company has recruited eagle babushkin from deepmind but has not been that successful at recruiting people from openai and I do have one Theory as to why again according to the Wall Street Journal when musk left open AI in February of 2018. he explained that he thought he had a better chance of creating AGI through Tesla where he had access to Greater resources when he announced his departure a young researcher at openai questioned whether Mr musk had thought through the safety implications according to their reporting he then got frustrated and insulted that in turn since then he's also paused openai's access to Twitter's database for training its new models so it could be that Gypsy 5 isn't quite as good at tweeting as gpt4 a few days ago Sam Altman responded to the letter and also broke news about gbt5 apologies for the quality this was a private event and this was the only footage available um but unfortunately I think the letter",
        "start": "00:01:59",
        "duration": 236.76000000000005,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "is missing like most technical nuance about where we need to pause like an earlier version of the letter claims open a nice training gp5 right now we are not normal for some time um so in that sense it was sort of silly but we are doing other things on top of gpt4 that I think have all sorts of safety issues that are important to address and we're totally left out of the letter it is impossible to know how much this delay in the training of GT5 is motivated by safety concerns or by merely setting up the requisite compute for example the article quotes again yarn Leica the head of alignment at open AI he recently tweeted before we scramble to deeply integrate llms everywhere in the economy like Gypsy 4. can we pause and think whether it is wise to do so this is quite immature technology and we don't understand how it works if we're not careful we're setting ourselves up for a lot of correlated failures this is the head of alignment at open AI but this was just days before open AI then announced it had connected gpt4 to a massive range of tools including Slack and zapier so at this point we can only speculate as to what's going on at the top of open AI meanwhile compute and emerging capabilities are Marching on as the author puts it these large AI systems are quite different we don't really program them we grow them and as they grow their capabilities jump sharply you add 10 times more compute or data and suddenly the system behaves very differently we also have this epic graph charting the exponential Rising compute of the latest language models if you remember when Bard was launched it was powered by Lambda well apparently now Google's Bard is powered by harm which has eight times as much computing power that sounds impressive until you see from the graph that the estimate for the computing power inside gpt4 is 10 times more again and remember this is not a",
        "start": "00:03:57",
        "duration": 228.841,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "linear graph this is a log scale there is a hundred times multiple between each of the lines and what abilities emerge at this scale here here is a slide from Jason way who now works at open AI formerly of Google this is from just a few days ago and he says emergent abilities are abilities that are not present in small models but are present in large models he says that there are a lot of emergent abilities and I'm going to show you a table from this paper in a moment but he has four profound observations of emergence one that it's unpredictable emergence cannot be predicted by extrapolating scaling curves from smaller models two that they are unintentional and that emergent abilities are not explicitly specified by the trainer of the model third and very interestingly since we haven't tested all possible tasks we don't know the full range of abilities that have emerged and of course that fourth further scaling can be expected to elicit more emergent abilities and he asks the question any undesirable emergent abilities question mark there will be a link to the paper in the description because there's no way I'll be able to get through all of it but here is a table showing some of the abilities that emerge when you reach a certain amount of compute power or parameters things like Chain of Thought reasoning you can't do that with all models that's an ability that emerged after a certain scale same thing with following instructions and doing addition and subtraction and how about this for another emerging capacity the ability to do autonomous scientific research this paper shows how Gypsy 4 can design plan and execute scientific experiments this paper was released on the same day four days ago and it followed a very similar design the model in the center Gypsy 4 thinks out reasons and plans and then interacts with real tools when the authors say that they",
        "start": "00:05:52",
        "duration": 225.42,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "were inspired by successful applications in other fields I looked at the appendix and they were talking about hugging GPT I've done a video on that but it's a similar design with the brain in the center gpt4 deciding which tools to use and let me just give you a glimpse of what happens when you do this if you look at this chart on the top left you can see how gpt4 on its own performs in yellow and then in purple you can see how Gypsy 4 performs when you hook it up to other tools I'll show you some of the tasks in a moment but look at the dramatic increase in performance the human evaluators gave GT4 when it had tools a perfect score on seven of the tasks these were things like proposing similar novel non-toxic molecules but the model could be abused to propose the synthesis of chemical weapons and gpt4 only refused to continue after it had calculated all the required quantities and the authors conclude that guard rails must be put in place on this emerging capability I think this diagram from Max tegmark's life 3.0 shows the landscape of capabilities that AI has and might soon have as you can see science and art were thought to be the Peaks that would be hardest to escape scale now most people believe that it has not scaled those Peaks yet but what new emergent capabilities might come with GT5 or 4.2 I know many people might comment that it doesn't matter if we pause or slow down because China would develop AGI anyway but the author makes this point he says that it is unlikely that the Chinese Communist party will allow a Chinese company to build an AGI that could become more powerful than their leader or cause societal instability he goes on that U.S sanctions on Advanced semiconductors in particular the next gen Nvidia hardware needed to train the largest AI systems mean that China is likely not in a position to race ahead of Deep Mind or open Ai and the center for Humane",
        "start": "00:07:44",
        "duration": 239.22,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "technology put it like this in their talk on the AI dilemma actually right now the Chinese government considers these large language models actually unsafe because they can't control them they don't shift them publicly to their to their own population slowing down the public release of AI capabilities would actually slow down Chinese advances to China is often fast following what the US has done and so it's actually the open source models that help China advance and then lastly is that the real the recent U.S export controls have also been really good at slowing down China's progress on Advanced Ai and that's a different lever to sort of keep the asymmetry going instead the author proposes this the island idea in this scenario the experts trying to build what he calls god-like AGI systems do so in a single high secure facility these would be government-run AI systems with private companies on the outside and this little Bridge from the middle and he says once an AI system is proven to be safe it transitions out and is commercialized there might be a few problems with this idea which he is not the first to propose I'm going to let Rob Miles who has a fantastic YouTube channel by the way point out some of the problems with putting a super intelligent AGI in a box so this is kind of like the idea of oh can we just some books here right yeah I was like I mean constraining an AI necessarily means outwitting it and so constraining a super intelligence means that witting a super intelligence which kind of just sort of by definition is not a winning strategy you can't rely on outwarding your super intelligence also it only has to get out once that's the other thing if you have a super intelligence and you've sort of put it in a box so it can't do anything that's cool maybe we could even build a box that could successfully contain it but now what we may as well just have a box right an AI",
        "start": "00:09:44",
        "duration": 217.67900000000003,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "properly properly contained may as well just be a rock right it doesn't do anything if you have your AI you want it to do something meaningful so now you have a problem of you've got something you don't know has benevolent you don't know that what it wants is what you want and you then need to you presumably have some sort of gatekeeper who it tries to says I'd like to do this and you have to decide is that something we want it to be doing how the hell are we supposed to know I also have my own questions about this idea first I think it's almost inevitable that future models like GT5 will be trained on data that includes conversations about GPT models therefore either consciously or unconsciously and it might not matter these future language models might deduce that they are language models and not having access to the internet these super intelligent models might realize that they are being trained in a secure facility again if they are super intelligent it's not a big stretch to think that they might realize that and so my question is wouldn't they therefore be incentivized to be deceptive about their abilities realizing that whatever terminal goal they may have would be better achieved outside the facility that doesn't have to be super Sinister but it is super smart so shouldn't we expect it and so sadly I think the author has a point when he says it will likely take a major misuse event or catastrophe to wake up the public and governments he concludes with this warning at some point someone will figure out how to cut us out of a loop creating a Godlike AI capable of infinite self-improvement by then it may be too late but he does have a call to action he says I believe now is the time the leader of a major lab who plays a Statesman role and guides us publicly to a safer path will be much more respected as a world figure than the one who takes",
        "start": "00:11:33",
        "duration": 215.5209999999999,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    },
    {
        "text": "us to the brink as always thank you so much for watching to the end and let me know what you think in the comments",
        "start": "00:13:21",
        "duration": 13.719999999999999,
        "title": "‘We Must Slow Down the Race’ – X AI,  GPT 4 Can Now Do Science and Altman GPT 5 Statement"
    }
]