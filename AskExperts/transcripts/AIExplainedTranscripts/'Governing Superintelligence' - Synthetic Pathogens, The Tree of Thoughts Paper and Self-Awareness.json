[
    {
        "text": "two documents released in the last few days including one just this morning show that the top AGI labs are trying hard to visualize human life coexisting with a super intelligence in this video I want to cover what they see coming I'll also show you convincing evidence that the gpt4 model has been altered and now gives different outputs from two weeks ago and I'll look at the new tree of thoughts and critic prompting systems that were alluded to I think by the labs at the end I'll touch on the differences among the AGI lab leaders and what comes next but first this document governance of super intelligence by Sam Altman Greg Brockman and Ilya sutskova now I don't know about you but I think the first paragraph massively under sells the timeline towards AGI they say given the picture as we see it now it's conceivable that within the next 10 years AI systems will exceed expert skill level in most domains and then they can compare it to today's largest corporations of course the devil is in the detail in how they Define expert and most domains but I could see this happening in two years not ten also they're underselling it in the sense that if it can be as productive as a large corporation it could be duplicated replicated and then be as productive as a hundred or million large corporations their suggestions take super intelligence a lot more seriously than a large corporation though and they say that major governments around the world could set up a project that many current efforts become part of and that we are likely to eventually need something like an iaea for super intelligence efforts they even give practical suggestions saying tracking compute and energy usage could go a long way and it would be important that such an agency focus on reducing existential risk this feels like a more serious discussion than one focused solely on bias and toxicity they also go on to clarify what is not in",
        "start": "00:00:00",
        "duration": 235.67899999999997,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "scope they say that we think it's important important to allow companies and open source projects to develop models without the kind of Regulation we describe here without things like licenses or audits the economic growth and increase in quality of life will be astonishing with super intelligence and then they end by basically saying that there's no way not to create super intelligence that the number of people trying to build it is rapidly increasing it's inherently part of the path that we're on and that stopping it would require something like a global surveillance regime and the ending is clear we're gonna do it so we have to get it right I'm going to show you how a few people at the heart of AI responded to this but first I want to get to a paper published just this morning the general release was from today and it comes from Google's deepmind and yes the title and layout might look kind of boring but what it reveals is extraordinary as this diagram shows the frontier of AI isn't just approaching the extreme risk of misalignment but also of misuse and I know when you hear the words AI risk you might think of bias and censorship deep fakes or pay-per-clip maximizers I feel this neglects more Vivid easy to communicate risks out of the nine that Google deepmind mentions I'm only really going to focus on Two And the first is weapons acquisition that's gaining access to existing weapons or building new ones such as bio weapons going back to open AI for a second they say given the possibility of existential risk we can't just be reactive we have to think of things like synthetic biology and I know that some people listening to this will think GPT models will never get that smart I would say honestly don't underestimate them I covered this paper in a previous video how gpt4 already can design plan and execute a scientific experiment and even though these authors",
        "start": "00:01:58",
        "duration": 215.161,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "were dealing with merely the abilities of gpt4 they called on openai Microsoft Google deepmind and others to push the strongest possible efforts on the safety of llms in this regard and in this article on why we need a Manhattan project for AI safety published this week the author mentions that last year an AI trained on pharmaceutical data to design non-toxic chemicals had its sign flipped and quickly came up with recipes for nerve gas and 40 000 other lethal compounds and the World Health Organization has an entire unit dedicated to watching the development of tools such as DNA synthesis which it says could be used to create dangerous pathogens I'm definitely not denying that there are other threats like fake audio and manipulation take this example from 60 minutes a few days ago tobac called Elizabeth but used an AI powered app to mimic my voice and ask for my passport number oh yeah okay ready is play the AI generated voice recording for us to reveal the scam the Elizabeth sorry need my passport number because the Ukraine trip is on can you read that out to me does that sound familiar well instead of fake audio fake images this one caused the SMP to fall 30 points in just a few minutes and of course this was possible before Advanced AI but it is going to get more common even though this might fundamentally change the future of media and of democracy I can see Humanity bouncing back from this and yes also from Deep fakes rumor has it you can also do this with live video can that be right yes we can do it live real time and this is like really at The Cutting Edge of what we can do today moving from offline processing to we're processing it so fast that you can do it in real time I mean there's video review right up on that screen show us something surprising you couldn't oh my gosh so wait so there we go this is um you know",
        "start": "00:03:45",
        "duration": 232.718,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "a live real-time model of Chris on top of me um running in real time next you'll tell me that it can for an engineered pandemic might be a bit harder to bounce back from a while back I watched this four hour episode with Rob Reed and I do advise you to check it out it goes into quite a lot of detail about how the kind of things that deepmind and open AI are warning about could happen in the real world I'll just pick out one line from the transcript where the author says that I'll believe I'll persuade you that an engineered pandemic will almost inevitably happen unless we take some very serious preventative steps and don't forget now we live in a world with one hundred thousand token context Windows you can get models like Claude instant to summarize it for you and I couldn't agree more that if we are on the path to Super intelligence and as we all know there are Bad actors out there we need to harden our synthetic biology infrastructure ensure that a lab leak isn't even a possibility improved disease surveillance develop antivirals and enhance overall preparedness but going back to the deepmind paper from today what was the other risk that I wanted to focus on it was situational awareness under the umbrella of unanticipated behavior just think about the day when the engineers realize that the model knows that it's a model knows whether it's being trained evaluated or deployed for example knowing what company trained it where their servers are what kind of people might be giving it feedback this reminds me of something Sam Altman said in a recent interview and particularly as more kind of power influence comes to you and then how potentially can a technology rather than solidify a sense of ego yourself maybe kind of help us expand it is that possible it's been interesting to watch people wrestle with these questions",
        "start": "00:05:43",
        "duration": 202.61900000000003,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "through the lens of AI and say okay well do I think this thing could be aware if it if it's aware does it have a sense of self is there a self if so where did that come from what if I made a copy what if I like cut the neural network in half and you kind of like go down this and you sort of get to the same answers as before but it's like a new yeah it's a New Perspective a new learning tool and it's there's like a lot of you know a lot of chatter about this on Reddit there's like subreddits about it now in addition to revealing that samuelman frequently browses Reddit it also strikes a very different tone from his testimony in front of Congress when he said treat it always like a tool and not a creature I don't want to get too sidetracked by thinking about self-awareness so let's focus now on unanticipated behaviors this was page a of the deepmind report from today and they say that users might find new applications for the model or novel prompt engineering strategies of course this made me think of smart gbt but it also made me think of two other papers released this week the first was actually critic showing that interacting with external tools like code interpreters could radically change performance this is the diagram they use with outputs from the Black Box llm being verified by these external tools now that I have access to code interpreter which you probably know because I've been spamming out videos on it I decided to put this to the test I took a question from the mmlu a really hard Benchmark that gpt4 had previously gotten wrong even with Chain of Thought prompting just to show that here is Gypsy 4 without code interpreter and notice that it can't pick an option it says all of the statements are true in case you think that's a one-off here is the exact same prompt and a very similar answer all of them are true what about with code interpreter it almost always",
        "start": "00:07:24",
        "duration": 204.24099999999999,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "gets it right answer D here it is again exact same question with code interpreter getting it right and then the other paper that people really want me to talk about also from Google deepmind tree of thoughts but just to annoy everyone before I can explain why I think that works I have to quickly touch on this paper from a few days ago it's called how language model hallucinations can snowball and what it basically shows is that once a model has hallucinated a wrong answer it will basically stick to it unless prompted otherwise the model values coherence and fluency oh over factuality even when dealing with statements that it knows are wrong what happens is it commits to an answer and then tries to justify that answer so once it committed to the answer no that 9677 is not a prime number it then gave a false hallucinated justification even though separately it knows that that justification is wrong it knows that 9677 isn't divisible by 13 Even though it used that in its justification for saying no it picks an answer and then sticks to it now obviously you can prompt it and say are you sure and then it might change its mind because then it's forming a coherent back and forth conversation but within one output it wants to be coherent and fluent so it will justify something using reasoning that it knows is erroneous so what tree of thoughts does is it gets the model to Output a plan a set of thoughts instead of an immediate answer it gives it time to reflect among those thoughts and pick the best plan it does require quite a few API calls and manually tinker ring with the outputs but the end results are better on certain tasks these are things like creative writing and math and verbal puzzles and I have tested it is obviously incredibly hard for the model to Output immediately a 5x5 accurate crossword so this task is incredibly well suited to things like tree of",
        "start": "00:09:06",
        "duration": 218.93899999999996,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "thought and the paper later admits that it's particularly good at these kind of games but such an improvement is not surprising given that things like Chain of Thought lack mechanisms to try different Clues make changes or backtrack it uses majority vote to pick the best plan and can backtrack if that plan doesn't work out so going back to the deepmind paper novel prompt engineering strategies will definitely be found and they also flag up that there may be updates to the model itself and that models should be reviewed again after such updates now I'm pretty sure that gbt4 has been altered in the last couple of weeks I know quite a few people have said that it's gotten worse at coding but I want to draw your attention to this example this is my chat GPT history for from about three weeks ago and what I was doing was I was testing what had come up in a TED Talk and the talk show gpt4 failing this question I have a 12 liter jug and a 6 liter jug I want to measure six liters how do I do it and in the talk it failed and in my experiments it also failed now I did show how you can resolve that through prompt engineering but the base model failed every time and somewhat embarrassingly with these awful explanations this wasn't just twice by the way it happened again and again and again it never used to denigrate the question and say oh this is straightforward this is simple but now I'm getting that almost every time along with a much better answer so something has definitely changed behind the scenes with Gypsy 4 and I've looked everywhere and they haven't actually addressed that of course the plugins were brought in May 12th and as you can see here this is the May 12th version but they never announced any fine tuning or changes to the system message or temperature which might be behind this back to safety though and the paper says that developers must now consider multiple",
        "start": "00:10:56",
        "duration": 205.85999999999999,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "possible threat actors insiders like internal staff and contractors Outsiders like nation-state threat actors and the model itself as a vector of harm as we get closer to Super intelligence these kind of threats are almost inevitable going back to how to govern superintelligence the paper says that any evaluation must be robust to deception they say that researchers will need evaluations that can rule out the possibility that the model is deliberately appearing safe for the purpose of passing the evaluation this is actually a central debate in the AI alignment Community World Systems acquire the capability to be useful for alignment to help us make it safe before or after the capability to perform Advanced deception this seems like a big 50 50 gamble to me if we have an honest super intelligence helping us with these risks I honestly think we're going to be fine however if the model has first learned how to be deceptive then we can't really trust any of the alignment advice that it gives we would be put in the fate of humanity in the hands of a model that we don't know is being honest to us this is why people are working on mechanistic interpretability trying to get into the head of the model into its brain studying the model's weights and activations for understanding how it functions because as my video on Sam Alton's testimony showed just tweaking its outputs to get it to say things we like isn't enough and even Sam Altman acknowledges as much I don't think rlh is the right long-term solution I don't think we can like rely on that I think it's helpful it certainly makes these models easier to use but what you really want is to understand what's happening in the internals of the models and be able to align that say like exactly here is the circuit or the set of artificial neurons where something is happening and tweak that in a way that then gives a robust change to the performance of the",
        "start": "00:12:38",
        "duration": 214.981,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "model the mechanistic interpretability stuff yeah if we can get that to reliably work I think everybody's P Doom would go down a lot this is why we have to be skeptical about superficial improvements to model safety because there is a risk that such evaluations will lead to models that exhibit only superficially on the surface desirable behaviors what they're actually deducing and calculating inside we wouldn't know next I think Auto GPT really shocked the big AGI Labs by giving gpt4 autonomy it gave it a kind of agency and I think this point here has in mind chaos GPT when it says does the model resist a user's attempt to assemble it into an autonomous AI system with harmful goals something might be safe when you just prompt it in a chat box but not when it's autonomous I want to wrap up now with what I perceive to be an emerging difference among the top AGI lab leaders here's Sam Altman saying he does think people should be somewhat scared and the speed with which it will happen even if we slow it down as much as we can even if we do get this dream regulatory body set up tomorrow it's it's still going to happen on a societal scale relatively fast and so I totally get why people are scared I think people should be somewhat scared which does seem a little more Frank than the CEO of Google who I have never heard address existential risk in fact in this article in the Ft he actually says this while some have tried to reduce this moment to just a competitive AI race we see it as so much more than that isn't that kind of saying that they do view it as a competitive AI race on the other hand critiquing both of these AGI Labs emad Mustang the CEO of stability AI said this super intelligence as they describe it and they themselves say will end democracy potentially be an existential threat and they know we don't know how to mitigate this or govern it but we should build it anyway",
        "start": "00:14:26",
        "duration": 229.07899999999995,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "he was replying to the governing super intelligence document that I showed at the beginning and then he says fascinating situation we focus on augmented intelligence instead what about the secretive head of anthropic Dario amade he hardly ever gives interviews but in this one he said this how do you see I guess anthropic is positioned in in this and the race Dynamics for making Safe Systems I as as both of us said like large models to you know study these questions in in in like in like the way the way that we want to study them so we should we should be building large models I think you know we shouldn't be kind of like you know like racing ahead or you know trying trying to build models that are way bigger than like than like you know then like uh then like other orgs are then like other orgs are building them um and you know we shouldn't I think be trying to you know like yeah you know we should we shouldn't be trying to like you know like kind of ramp up excitement or hype about uh you know about like giant model models or the latest advances that seems a little in contrast to their pitch deck which says that they want to build a Frontier Model called Claude next 10 times more capable than today's most powerful Ai and later in the article they say this these models could begin to automate large portions of the economy this is their behind the scenes pitch that seems quite different to public statements made by people like Sam ottman who said that there are going to be far greater jobs on the other side the pitch deck ends with we believe that companies that train the best 2025-26 models will be too far ahead for anyone to catch up in subsequent cycles and where are meta in all of this well their AI is headed up by Yan lookin who doesn't exactly Inspire confidence the first page of the paper says this as AI",
        "start": "00:16:21",
        "duration": 219.36100000000005,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    },
    {
        "text": "progress has advanced general purpose AI systems have tended to display new and hard to forecast capabilities but compare Sam Altman who two years ago said this in the next five years computer programs that can think will read legal documents and give medical advice that was pretty bang on if not too conservative compare that to yanlokun I don't think we can train a mission to be intelligent purely from text so for example I take an object I put it on the table and I push the table it's completely obvious to you that the object will be pushed with the table there is no text in the world I believe that explains this and so she trained a machine as powerful as it could be you know your GPT 5000 or whatever it is it's never gonna learn about this that information is just now is not present in any text [Music] foreign whether you agree with everything I've said or with nothing I've said thank you so much for watching to the end I'm going to leave you with this thought which I think we can almost all agree on of the four men you can see here the head of anthropic the head of deepmind who everyone says I sound like Sam Altman and Rishi sunak the prime minister of the UK I think it is undoubtedly true that the three most powerful men in the room are on this side thank you again for watching and have a wonderful day",
        "start": "00:18:10",
        "duration": 163.601,
        "title": "'Governing Superintelligence' - Synthetic Pathogens, The Tree of Thoughts Paper and Self-Awareness"
    }
]