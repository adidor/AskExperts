[
    {
        "text": "just a few hours ago a host of AI industry leaders experts and academics put out this 22-word statement on making AI safety a global priority the so-called statement on AI risk brought together for the first time all the current AGI lab leaders that's people like Sam Altman Ilya satskova Demus hasabis and Dario Amadeus and two of the three founders of deep learning itself Joshua bengio and Jeffrey Hinton here is what the statement said mitigating the risk of Extinction from AI should be a global priority alongside other societal level risks such as pandemics and nuclear war it is now almost impossible to deny that this is now the consensus view among AI experts let's first look at the Preamble then break down the statement and show you the signatories they say that AI experts journalists policy makers and the public are increasingly discussing a broad spectrum of important and Urgent risks from AI even so it can be difficult to voice concerns about some of the advanced ai's most severe risks the succinct statement below aims to overcome this obstacle and open up discussion it is also meant to create common knowledge of the growing number of experts and public figures who also take some of advanced ai's most severe risks seriously the first point is that the statement is in a way optimistic it says we can mitigate this risk perhaps not eliminate it but mitigate it reduce the risk second it says we should do this globally and that's not just among all the different AGI Labs almost all of which sign these statement but also between countries in that vein there were quite a few prominent signatories from China and the third point that I'd make is that they put it on a par with pandemics and nuclear war toward the end of the video I'll show you that's not as far-fetched as it sounds but anyway who actually signed this statement let's find out we have two of the three founders of deep",
        "start": "00:00:00",
        "duration": 237.12000000000003,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "learning that's Jeffrey Hinton and Joshua bengio the third founder was yanlichun and we'll touch on him later in the video all three of those won the most prestigious Accolade in computer science which is the Turing award then we have three of the CEOs of the top AGI Labs Sam Altman Demis hasabis and Dario amade of openai Google deepmind and anthropic none of those signed the pause letter but they did sign this statement and actually as interestingly for me so did Ilya satsukova who I see as the brains behind open AI he of course also worked with Jeffrey Hinton on deep learning and is widely regarded as the smartest guy in machine learning you will also notice so many Chinese signatories especially from xinhua University which I've actually visited in China it's one of their leading universities that's a really encouraging sign of cooperation between the west and countries like China on AI and the list of significant signatories goes on and on and on these are senior people at the top of Deep Mind anthropic and open Ai and there are names like Stuart Russell who wrote The Textbook on AI who also signed the pause letter let me highlight a few more names for you here you have the CTO of Microsoft itself Kevin Scott he's the guy who basically heads up the partnership between openai and Microsoft I think many people will miss his name but I think it's particularly significant that he also signed this notice also the CEO of stability AI emad mostacc the center for AI safety coordinated this effort and I'll get on to their eight examples of AI risk in a moment but first let's pick out a few more names you've got David Chalmers Daniel Dennett Lex Friedman and Victoria krakovna now together with the statement the center for AI safety also puts out this eight examples of AI risk I've read almost every paper linked to in these eight examples so I'm going to try to summarize them fairly briefly because I",
        "start": "00:01:59",
        "duration": 239.15999999999997,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "know not everyone will will be that interested it starts by saying that AI could be profoundly beneficial but also present serious risks due to competitive pressures before we get to the risks I want to touch on some of the upsides recently outlined by Demis hasabis and these showcase what can happen if we get this right we had sort of a golden couple of years in some sense for AI for science we've had lucky enough to have many Nature and Science papers published in all sorts of domains so from quantum chemistry better DFT functions to approximate Schrodinger's equation pure mathematics we've solve some important conjectures in topology with collaborating with some brilliant mathematicians who found working on Fusion reactors with epfl on their test Fusion reactor controlling the plasma in real time in their Fusion reactors and being able to hold the plasma safely in place for for arbitrary amounts of time being able to predict rainfall many hours ahead and more accurately than current meteorological models and then in applications there's a ton of those two we saved one of the any things we did at Google was saving about 30 of the cooling energy used to cool the massive data centers at Google so there's a huge energy saving and we're starting to explore doing that across actual whole power grids and this Echoes what Joshua bengio said in a recent blog post which is that we can build immensely useful AI systems that are modeled after ideal scientists and do not act autonomously in the real world janlecon recently said that we would never give current llms agency there is a a flaw in current Auto reversive lens so there is no persistent memory first of all but second of all you cannot control the system you cannot impose constraints on it like be factual be understandable by a 13 year old and that makes them very difficult to to control and steer and so that creates some fears because people are kind of",
        "start": "00:03:58",
        "duration": 217.97899999999993,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "extrapolating if we let those systems do whatever we connect them to internet and they can do whatever they want they're going to do crazy things and stupid things and perhaps dangerous things and we're not going to be able to control them and they're going to escape Troll and they're going to become intelligent just because they're bigger right and that's nonsense first of all because this is not the type of system that we are going to give agency to that was a week before this paper was published on the results of giving agency to current large language models the paper showed that current llms with agency are able to utilize the learn skill library in Minecraft to solve novel tasks from scratch zooming into the diagram you can see how this Voyager model outperforms reflection which I've talked about in previous videos and auto GPT and it discovers new items and skills continually by self-driven exploration significantly outperforming the bass lines indeed Andre carpathy responded to this study saying very clear that AGI will Mega transform Society but still will have but is it really reasoning how do you define reasoning oh it's only predicting the next token can machines really think and he called that armchair philosophy previously though even Yan lacun has admitted some risks saying you know it's like rockets you test it it blows up you tweak it and then try again I'm not sure I'm okay with an attempt at AGI blowing up the first time but I'll leave that up to you to decide so what are these eight examples of AI risk that the center for AI safety to organize the statement list out they say that AI systems are rapidly becoming more capable they can power autonomous weapons promote misinformation and conduct cyber attacks as we've seen they are increasingly able to act autonomously now there is so much to say here and I've read each of these but I want to keep it to just the highlights",
        "start": "00:05:47",
        "duration": 220.04000000000002,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "so let's move on to the first example weaponization malicious actors could repurpose AI to be highly destructive presenting an existential risk in and of itself and increasing the probability of political destabilization they talk about aerial combat and then building chemical weapons which I mentioned in my previous video on governing super intelligence then they mentioned developing AIC systems for automated cyber attacks they mentioned military leaders discussing giving AI systems decisive control over nuclear silos I'm going to quickly try to demonstrate why that kind of autonomous AI might not be such a good idea I want you to meet a hero stanislav Petrov he was the duty officer at the command center the OKO nuclear early warning system when the system reported that a missile had been launched from the US followed by up to five more Petrov judged the reports from the system to be a false alarm and his subsequent decision to disobey orders against Soviet military protocol is credited with having prevented an erroneous retaliatory nuclear attack on the U.S which it says could have resulted in a large-scale nuclear war which could have wiped out half the population of these countries involved an investigation later confirmed that the Soviet satellite warning system the machines behind it had indeed malfunctioned I would not have wanted that system to be or autonomous then we hear that gpt4 was able to autonomously conduct experiments and synthesize chemicals in a real world lab again I covered that paper at the time and then linking back to Petrov they say an accident with an automated retaliation system could rapidly escalate and give rise to a major war but that unlike previous weapons AI systems with dangerous capabilities could be easily proliferated through digital means hopefully you can start to see why we need to balance risks with opportunities",
        "start": "00:07:37",
        "duration": 226.72100000000003,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "but let's move on to misinformation I think we can all agree that we already have too much misinformation so let's move on to the next one which is proxy gaming this has already been showcased in the social dilemma where AI recommender systems are trained to maximize watch time and click rate metrics and this can lead people into Echo chambers that helps develop extreme beliefs in order to make those people easier to predict by the AI recommender systems so you might think it will be simple just to tell the AI to promote happiness or economic growth but that might not work out as you intend next is in feebleman if we delegate more and more tasks to machines we become increasingly dependent on them and here they actually mention the film Wally which if you remember the ending features this quite comically imagine if it becomes well known that companies led by AI CEOs bring in more profit well then it wouldn't take long for all companies to be under immense pressure to make their managers and CEOs Ai and I know what many people will be thinking couldn't that be an improvement on the current system and while I know exactly what you mean in the current world realistically it would still be the people owning the company that would derive the profit and while the ultimate answer may be some form of universal basic income we do need some time to set that up and the current accelerated AI arms race doesn't give us much of that time next is value lock-in which links very much to the last point about giving small groups of people a tremendous amount of power in other words if you want massive change to the way the world Works giving current leaders AGI might not be the best way of doing it they say that such AI systems might enable regimes to enforce narrow values through pervasive surveillance and oppressive censorship next is emergent goals this is sometimes called misalignment and",
        "start": "00:09:30",
        "duration": 220.799,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "we've already seen many AI agents develop goals such as self-preservation and you can see why even a system designed to do good might have that goal you can't do good and help the world if you're shut down so it makes sense that even the most benign AI might want to preserve itself and to take actions including through deception to make sure that it's not shut off and this is not just Theory the accompanying academic paper natural selection favors AIS over humans gave this example agents could behave One Way during testing and another way once they are released to win the war game diplomacy which many of you will have heard of players need to negotiate J form alliances and become skilled at Deception to win control of the game's economic and Military resources AI researchers have trained metas AI agent Cicero an expert manipulator to do the same in summary it would cooperate with a human player then change its plan and backstab them in Future these abilities could be used against humans in the real world again that's not because they're malevolent or hate humans it just makes sense it's smart to do so this brings us neatly on to deception and they give the great example of Volkswagen who programmed their engines to reduce emissions only when being monitored and future AI agents could similarly switch strategies when being monitored and take steps to obscure their deception from monitors once deceptive AI systems are cleared by their monitors or once such systems can overpower them these systems could take a treacherous turn and irreversibly bypass human control I talked a bit more about that point of when AI might become deceptive in my previous video on governing super intelligence it is a key debate in the AI alignment Community about whether models will become deceptive before they become helpful for alignment but finally we have power seeking behavior and this example ends",
        "start": "00:11:21",
        "duration": 221.82000000000002,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    },
    {
        "text": "on this dark note building power seeking AI is also incentivized because political leaders see the Strategic advantage in having the most intelligent most powerful AI systems for example Vladimir Putin has said whoever becomes the leader in AI will become the ruler of the world so those were the eight examples and yes I would have signed this statement but I'm not a significant figure so I can't anyway let me know in the comments if you agree that this should be a global priority and of course you can also let me know if you don't think it should be a global priority my goal in this channel is to cover both the risks and opportunities so I'd love to hear from you whatever your opinion is have a wonderful day",
        "start": "00:13:12",
        "duration": 80.201,
        "title": "Hassabis, Altman and AGI Labs Unite - AI Extinction Risk Statement [ft. Sutskever, Hinton + Voyager]"
    }
]