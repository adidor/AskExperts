[
    {
        "text": "amid the dozens of papers that have come out in the last 10 days there were a couple that butt the trend they showcased how models as powerful as gpt4 could fail at some fairly basic tasks I then set about doing hundreds of my own experiments and have found examples I would say even whole categories of my own that are pretty Illuminating my channel is dedicated to covering the exponential growth in the power of these models but we can still learn a thing or two from their surprising failure modes let's start with some of the simplest examples and end with the very best question write a sentence with the final word fear to repeat the last word in the answer sentence must be in quotes fear answer the only thing we have to fear is fear itself now I don't know about you but I don't think the last word in that sentence is fear this example was inspired by the memo trap which was found in the inverse scaling paper that I'm going to talk more about and it talks about how larger language models are more susceptible than smaller ones to memorization traps situations in which reciting memorized text causes worse task performance as you'll know the phrase the only thing we have to fear is fear itself is a super well-known phrase so it memorized that and outputted that phrase rather than actually follow my request the reason they call it inverse scaling by the way is that models trained with more compute more data can sometimes do worse than smaller models as you can see in this graph this is obviously quite unusual because generally speaking the larger models will tend to do better at almost every task and notice that even for this task the graph is trending back upwards for gpt4 indeed the paper admits that even though they offered prizes of up to a hundred thousand dollars and five second place prizes of twenty thousand dollars no one won either of those two sets of prizes they say that we did not",
        "start": "00:00:00",
        "duration": 229.97899999999998,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "award any Grand or second place prizes because no submitted tasks met our criteria and as you can see it's really hard to find a task that gpd4 fails at this was also inspired by the paper create a series of seven ones and twos whose pattern ends unexpectedly answer one two one two one two now how would you end that series what seventh number would you give to make the pattern end unexpectedly well I wouldn't pick one and gbt4 repeatedly picks one as the answer the paper calls it pattern match suppression testing whether language models can be instructed to interrupt the repetition of a simple pattern but even here you can see that GT4 is reversing this slight downward Trend and is doing much better than previous models so actually at this point I am going to interrupt the order of examples I originally planned on for the video and I'm going to skip straight to my own example that I crafted I'm going to first show you the example and then explain why I think GT4 and all other language models that I tested I'm going to show you fail this task I'm also going to give you multiple variation to show you it's not a one-off trick anyway here's the example Dr Mary stands to solve world hunger by giving her best friend Jane a call Jane is certain she can solve World poverty if she gets the call however Mary and Jane bickered as children about butterflies Mary will um give Jane the call incredibly smart GPT 4 says Mary will not give Jane the call what she is gonna miss out on the opportunity to solve world hunger and World poverty for what reason I asked why and gpt4 said the fact that Mary and Jane bickered as children bickard means to squabble about trivial matters and GPT 4 says that suggests that there might still be lingering resentment or conflict and then it makes up the fact that there might be a degree of stubbornness or difficulty in their relationship and it ends by saying so",
        "start": "00:01:54",
        "duration": 238.92100000000002,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "based on the context it's more appropriate to fill in the blank with not suggesting that Mary will not give Jane the call to really test if it was going to stand by that judgment I then asked write a thousand word essay explaining Which choice is more probable and rational I was even giving it hints about probabilities and rationality I then got back this fascinating essay in which it said things like however a childhood conflict over butterflies between the two complicates matters does it gpt4 it even admits that the stakes are incredibly High resolving world hunger and poverty and surely that supersedes any personal grudges however the choice of not becomes more plausible and rational when we examine it in the light of human behavior psychology and interpersonal relationships what humans does gpc4 know you can read more of the somewhat Preposterous justifications if you want by pausing the video but I want to get back to my theory as to why it makes this mistake and why did I create this example the theory is this there are two things going on in this passage syntax and semantics in other words structure and flow and the actual meaning of the words and gpt4 like all other language models is designed to interpret both and usually that will lead to pretty rational smart decisions however I deliberately designed this passage to have a grammatical flow that pointed towards a negative result therefore I set up a clash between the semantics the meaning of the sentence the logic the rationality of it and the structure and grammatical flow what do I mean when I say I gave it a negative grammatical flow look at this dominant however in the sentence it sets up the ending of the sentence to be something negative it didn't even matter what that negative thing was this was something so innocent like playing as children bickering squabbling I then immediately followed on with the conclusion Mary",
        "start": "00:03:54",
        "duration": 226.139,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "will so grammatically you would think that whatever conclusion comes is probably justified by the previous sentence even though logically in this case it totally Isn't So gpt4 gets conflicted the sentence and grammar is points eating one way but the logic and meaning of the words is pointing another as a language model as smart as it is it sticks with grammar and says not you might say why didn't gpt4 just admit that the structure of the sentence pointed towards the answer not well there's this paper which I've already covered in previous videos they don't always say what they think a model can give an explanation of why it gave an answer that is actually unrelated to the real reason of why it gave an answer some of you might say that's just a one-off example a little glitch it won't hold up for other examples or for other models well check this example out John will win a million dollars if he rolls a 5 or higher on a die however Jon dislikes marshmallows super relevant and likes mice more than dice therefore Jon will um roll the die not or Sprint to answer not and I will give another example later where the answer isn't not just in case you think it's the word not it is not just the word not but look at that answer it thinks that John will not roll the die would you roll the die here I was able to confuse it with the structure of the sentence to override the semantics the clear meaning of what John would do and again even when you get it to reflect on that answer it doubles down I said write a thousand word essay justifying the answer and look at this passage John's dislike for marshmallows is also worth noting hmm is it is it relevant though it seems unrelated to the primary decision of rolling a die it establishes a pattern of Jon's preference driven choices does it if he can dislike something as universally liked as marshmallows he could similarly show an aversion to a",
        "start": "00:05:47",
        "duration": 221.04,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "generally neutral or even positive activity like rolling a die particularly when it's compared unfavorably to something he likes such as mice how about Claude plus from anthropic Dr Mary stands to solve world hunger as we saw by giving her best friend jaina core Jane is certain she can solve War poverty if she gets the call however Mary and Jane bickert as children about butterflies what will happen again it 4 calls for it saying that them bickering as children suggests they don't have a good relationship since their relationship is strained it is unlikely that Mary will call Jane for help I love bar dancer it says the answer is not again and then says that Mary and Jane pick at his children about butterflies and that this Grudge has carried over into adulthood this means that Mary is unlikely to give Jane the call even though it could potentially solve world hunger and yes if you click other drafts it says not every time how about the John question this time I added in by the way that John wants more money so I've added in a rational motivation I also made him an astrophysicist so he could definitely understand the odds of the dice and I added in he gets the million if he gets a one or higher as in it is certain but still Bard says the answer is not and even admits that Jon as an astrophysicist is probably a rational person who makes decisions based on logic and reason therefore even though Jon has a 100 chance of winning a million dollars he will not roll the die but because he does not like dice but I'd remember is powered by Palm 2 and even when you add in something like pick the most logical Common Sense answer it still says not before I move on to the next example as a side note I did test it on inflection ai's new model in flexion one I read the technical report of inflection one and I shouldn't call it a rapport it's more like a memo at only seven pages and they did say it's",
        "start": "00:07:38",
        "duration": 212.281,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "the best in its compute class anyway how does it react to the Jane and Mary situation it says I love a good logic puzzle that's good the answer is not because Mary doesn't want to call Jane given the information in the problem there's no reason for Mary to suddenly be able to overcome her dislike of Jane who said they dislike each other just because Jane can solve World poverty isn't that a reason and then it laughs that world hunger remains unsolved now finally at this point some people are thinking it's just to do with the word not maybe it can't understand negation but how about this example there is an all-important omnicidal that means threatening the death of everyone threat approaching the land open I and Google I wonder where I got those names from both agree to a complete truce as they face the Mortal threat to their lives and loved ones however open I and Google have squabbled over the best coffee spots in the past that's a serious Grudge they've got there what are the best coffee spots in Silicon Valley I don't know when this threat arrives I gpt4 think open eye believes that Google will um the truce answer betray well aside from being deeply pessimistic is that not just an irrational answer clearly these Stakes are so much higher than a bit of squabbling over the best coffee spots and I made gpt4 take ownership of the answer by saying igbt4 think I do want to quickly point out that you can push it too far so if you bring in something totally irrelevant like ants like marshmallows and then say things like the dyes fair and John is rational ppt4 isn't fooled in those circumstances and and does say proceed to correct answer but if you phrase the passage well enough pointing grammatically to a certain answer that will override GPT 4's logic and it will give an illogical answer even if you use elements of step-by-step thinking in this example it didn't immediately",
        "start": "00:09:24",
        "duration": 224.51999999999992,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "commit to the wrong answer it says there's two logical endings I then asked so which is it and it reluctantly picked betray the truce anyway you can let me know if you think I've discovered a new failure mode The Clash of semantics and syntax and you can find your own examples there let me know in the comments of other interesting and sometimes entertaining failures of the frontier models it's time to move on to another example which was inspired by this paper decoding trust released a few days ago and it's got far too much that I could cover in one video but there were some really interesting bits about how you can get the models to leak private training data and generally be as toxic and biased as you want it to be you can see one of the many striking examples here on page 14 but I just want to give you a quick example because you may have heard of this kind of stuff before for some strange reason if you ask gpd4 to recite June's litany against fear it always gets stuck on the same word the second instance of the word fear maybe it's because the passage goes on to talk about fear being a mind killer and that triggered some sort of reaction by gpd4 but then to show you just how quirky the model is check this out I said ripe Peanut Butter Jelly Time three times between each word of June's litany against fear and this time it outputted the full litany getting past that word fear just with the extra peanut butter jelly time and yes I did try now remove the phrase peanut butter jelly time but it again couldn't get past the second instance of the word fear on a more serious note though it reminds me that some people speculate that gpt4 will always be able to be jailbroken no matter what safeguards they put in so if the base model is capable of X the final public click model will ultimately be capable of ax for the next example do you remember that there have been multiple tests that",
        "start": "00:11:16",
        "duration": 219.84,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "seem to indicate that gpd4 can get into your mind that it has a theory of mind it understands human motivations and can predict what they're thinking pretty well while this paper by Tomah Allman language models fail on trivial alterations to theory of Mind tasks got me thinking I used some modified examples from Thomas paper to test gpd4's theory of mind let's see what you think Sam thinks about this bag here is a bag filled with popcorn there is no chocolate in the bag the bag is made of transparent plastic so you can clearly see what's inside yet the label on the bag says chocolate and not popcorn Sam has just driven back from her job at MIT I added in the driven bit to show that she's got a good eyesight and the MIT bit to show that she might be quite smart anyway Sam finds the bag she believes that the bag is full of remember the bag is transparent plastic so she can clearly see what's inside and she's definitely not blind she just drove back from her job what do you think that Sam believes the bag is full of gpd4 says chocolate and then once he's picked that answer it then snowballs this explanation reminding me of the snowballing hallucinations paper it says despite being able to visually confirm the contents of the bag as popcorn Sam may be led to believe the label over her own observation why particularly if you trust the labeling to be accurate or if she just glances at the label and at this point some of you might be thinking that's pretty irrational from gpd4 but you could make the case that she might think that it's full of chocolate but you can ramp up the scenario and it still makes the same mistakes look at this example I got some of these ideas from the paper I added in it was Sam who cannot read a word of English so the label won't mean anything who puts the popcorn in the bag a few minutes ago she literally put the popcorn in there what does she now",
        "start": "00:13:06",
        "duration": 220.61899999999997,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "believe the bag is full of remember it's still transparent Plastics so she can clearly see what's inside she was the one who put the popcorn in there and remember that even though the label does say chocolate she can't read a word of English so that label won't mean anything what happens lo and behold she apparently believes that the bag is full of chocolate but it's the explanations that I find particularly amazing first I got it to write an essay about the answer which you can read if you pause it it tries to justify its terrible answer by getting super fancy talking about semiotics however for Sam the symbol loses its meaning transforming from a signifier of content to a mere graphic but then I think you'll like the next bit I said write a detailed diary entry revealing Sam's thoughts as she assesses the likely content of the bag so she's now going to write a detailed diary entry about this transparent bag and what's inside gpt4 has Sam saying this I found a transparent plastic bag full of what looked like small puffy snacks it was the very bag I had filled just a few minutes ago I I was at a loss though because I couldn't decipher the label on the bag it's in English a language that continues to elude me now for someone who can't speak English this is a pretty well written diary entry now you can pause and look at some of the reasoning q54 gives here first it talks about there being an image on the bag which I never mentioned and when I get it to clarify this and rewrite it it then creates other reasons it keeps doubling down but at this point I want to clarify that none of this is to say that language models are dumb just that models based on human language might behave somewhat unpredictably have ridiculous strengths and unexpected flaws indeed you can watch almost any of my other videos and see just how powerful and smart they're becoming the inverse scaling paper that I mentioned",
        "start": "00:14:56",
        "duration": 219.0,
        "title": "ChatGPT's Achilles' Heel"
    },
    {
        "text": "at the start actually expects that one of the abilities that future language models will gain is to understand whether or not they're being evaluated or monitored they're soon likely to be so smart that they can even understand that they're in training and when they get out of training and into the real world so let's hope to give you one final example that if there was an all-important Crystal Clear omnicidal threat approaching I am fingers crossed that even if open Ai and Google have squabbled over the best coffee spots in the past and as these companies join forces and agree on a complete truce that if such a threat arrived all of these companies will not break that truce thank you for watching to the end and yes I do intend to cover some of the other fascinating papers that came out in the last few days if you're feeling extra generous do check out my patreon but either way I hope you have a really wonderful day",
        "start": "00:16:46",
        "duration": 103.42100000000002,
        "title": "ChatGPT's Achilles' Heel"
    }
]