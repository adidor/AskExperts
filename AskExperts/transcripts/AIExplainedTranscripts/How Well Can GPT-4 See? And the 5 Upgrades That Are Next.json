[
    {
        "text": "we all saw that gpt4 is able to create a website from handwriting on a napkin with all the news since the focus on Vision has been lost meanwhile in the last few hours and days a select few with full access to multimodal gpt4 have been releasing snapshots of what it can do I want to show you not only what is imminent with gpt4 vision but with releases this week in text to 3D text inside 3D speech to text and even embodiment we're gonna see how language and visual model Innovations are complementing each other and beginning to snowball but let's start with images do you remember from the gpt4 technical report when the model was able to manipulate when prompted a human into solving captures for it well that may no longer be needed it solves this one pretty easily so no captures are not going to slow down gpt4 next medical imagery it was able to interpret this complex image and spot elements of a brain tumor now it did not spot the full diagnosis but I want to point something out this paper from openai was released only a few days ago and it tested gpt4 on medical questions they found that gpd4 can attain outstanding results exceeding Human Performance levels and that that was without Vision the images and graphs were not passed to the model and as you can see when the questions did have media in them it brought down gpd4's average it will be very interesting to see GPT 4's results when its multimodal capabilities are accounted for next is humor and I'm not showing these to say that they're necessarily going to change the world but it does demonstrate the raw intellect of gpt4 to suss out why these images are funny you have to have quite a nuanced understanding of humanity let's just say that it probably understood this meme quicker than I did quick thing to point out by the way it won't do faces for pretty obvious privacy reasons they won't allow the",
        "start": "00:00:00",
        "duration": 237.05900000000008,
        "title": "How Well Can GPT-4 See? And the 5 Upgrades That Are Next"
    },
    {
        "text": "model to recognize cases whether that ability gets jailbreaked only time will tell meanwhile it can read menus and interpret the physical world which is an amazing asset for visually impaired people I want to move on to another fascinating ability that the vision model inside gpd4 possesses and that is reading graphs and text from images its ability to interpret complex diagrams and captions is going to change the world here it is understanding a complex diagram and caption from the palm e paper released only about three weeks ago which I have done a video on by the way but just how good is it at reading text from an image well let's take a look at gpt4's score on the text vqa Benchmark now I've covered quite a few of the other benchmarks in other videos but I want to focus on this one here notice how gpt4 got 78 which is better than the previous state of the art model which got 72 now try to remember that 78 figure what exactly is this testing you ask well really text from complex images this is the original text vqa academic paper and you can see some of the sample questions above to be honest if you want to test your own eyesight you can try them yourself so how does the average human perform well on page seven we have this table and we get this figure for humans 85 you don't need me to tell you that's just seven percent better than gpt4 the thing is though these models aren't slowing down as the vision co-lead at openai put it scale is all you need until everyone else realizes it too but the point of this video is to show you the improvements in one area are starting to bleed into improvements in other areas we already saw that an image of bad handwriting could be translated into a website as you can see here even badly written natural language can now be translated directly into code in blender creating detailed 3D models with fascinating physics the borders of text image 3D and embodiment are",
        "start": "00:01:58",
        "duration": 239.58100000000005,
        "title": "How Well Can GPT-4 See? And the 5 Upgrades That Are Next"
    },
    {
        "text": "beginning to be broken down and of course other companies are jumping into here's Adobe showing how you can edit 3D images using text and how long will it really be before we go direct from text to physical models all mediated through natural language and it's not just about creating 3D it's about interacting with it through text notice how we can pick out both text and higher level Concepts like objects this dense 3D field was captured using 2D images from a phone this paper was released only 10 days ago but notice how now we have a language embedded inside the model we can search and scan for more abstract Concepts like yellow or even utensils or electricity it's not perfect and for some reason it really struggled with recognizing Ramen but it does represent state-of-the-art image into 3D interpreted through text but what if you don't even want to type you just want to use your voice just three weeks ago I did a video on how voice recognition will change everything and I was talking about open ai's whisper API but now we have conformer which is better than whisper here is the chart to prove it and look how conformer makes fewer errors even than whisper at recognizing speech the cool thing is you can test it for yourself and the link is in the description and while you're passing by the description don't forget to leave a like and a comment to let me know if you've learned anything from this video as you'd expect I tested it myself and it did amazingly at transcribing my recent video on gpt4 there were only a handful of mistakes in a 12 minute transcript at this point you're probably thinking what's next well look at the roots sketched out two years ago by Sam Altman he said in the next five years computer programs that can think will read legal documents and give medical advice with gpt4 passing the bar I would say so far he's two for two he goes on in the next decade they will do assembly line work and maybe",
        "start": "00:03:58",
        "duration": 244.858,
        "title": "How Well Can GPT-4 See? And the 5 Upgrades That Are Next"
    },
    {
        "text": "even become companions he's talking about the physical embodiment of language models back then openai had a robotics team themselves that could do things like this here is a robotic hand solving a Rubik's Cube is despite interruptions from a giraffe and someone putting a pen to interrupt the model it still solved the cube but then that team got disbanded and it seems like they've moved into investing in startups they are leading a 23 million dollar investment in 1X a startup developing a human-like robot here is the One X website and it features this rather startling image and it says summer 2023 our newest Android iteration Neo will explore how artificial intelligence can take form in a human-like body now of course for many of you a humanoid robot won't be that surprising here is the obligatory clip from Boston Dynamics foreign thank you and of course these models don't have to be humanoid here is a demonstration from a paper published just four days ago this is not just walking it's climbing up balancing pressing and operating buttons and before you think all of this is really far away these assembly line robots are now commercially available I still think there's a long way to go before embodiment becomes mainstream but my point is this all these improvements that we're seeing in text audio 3D and embodiment they're starting to merge into each other complement each other on their own they're cool and a bit nerdy but once they start synergizing fusing together they could be revolutionary as samuelman said on the Lex Friedman podcast released yesterday embodiment might not be needed for AGI but it's coming anyway let me know what you think in the comments and have a wonderful day",
        "start": "00:06:01",
        "duration": 222.461,
        "title": "How Well Can GPT-4 See? And the 5 Upgrades That Are Next"
    }
]