[
    {
        "text": "in a somewhat provocative new interview with Wired Magazine Demis hasabis head of Google deepmind is quoted as saying that Gemini which could be released as soon as this winter will be more capable than open ai's Chachi PT he reveals that they are attempting to combine some of the strengths of alphago type systems with the amazing language capabilities of large models before we look into how that might work here is the context of the Gemini announcement from Sundar pichai they are focused on building more capable systems safely and responsibly this includes our next Generation Foundation model Gemini which is still in training while still early we are already seeing impressive multimodal capabilities not seen in Prior models as the best promises that we also have some new innovations that are going to be pretty interesting and I know many people will dismiss this as all talk but remember deepmind was behind not just Alpha go but also Alpha zero which can play any two-player full information game from scratch they were also behind Alpha style which conquered Starcraft 2 with quote long-term planning and let's remember that for later and most famously perhaps a Sabbath LED them to the incredible breakthrough of alpha fold and Alpha fold 2 which are already impacting the fight against plastic pollution and antibiotic resistance so let's not underestimate deepmind but back to Gemini we hear from the information recently that the multi-modality of Gemini will be helped in part by training on YouTube videos and apparently YouTube was also mined by openai of course that's not just the text transcripts but also the audio imagery and probably comments I wonder if Google deepmind might one day use YouTube for more than that a few days ago they released this paper on robocad which they call a self-improving foundation agent for robotic manipulation and the paper says that",
        "start": "00:00:00",
        "duration": 241.5,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "with Robocat we demonstrate the ability to generalize to new tasks and robots both zero shot as well as through adaptation using only a hundred to a thousand examples for the Target task we also show how a trained model itself can be used to generate data for subsequent training iterations thus providing a basic building block for an autonomous Improvement Loop notice that part about using the model itself to generate data that reminded me of a conversation I had with one of the authors of the textbooks are all you need paper Ronan eldan from Microsoft I'm making a video on their new Phi 1 model for coding we had a really great chat and we were discussing at one point AGI timelines and I said this when you get Elite math papers with proofs and Elite scientific research if you train on much more of those for way more epochs I don't think we're that far away from AGI I personally can't see any barrier within the next five years Ronan said this as you said I also don't see any barrier to AGI my intuition is that there's probably a lot more Improvement we can do with the data we have and maybe a little bit more synthetic data and this is even without starting to talk about self-improving mechanisms like Alpha zero where the more you train models with some verification process and you generate more data this can be done in math and other things as we see here with Robocat so you know there's just so many directions where we can still go that I don't think we're going to hit a ceiling anytime soon can't wait to show you guys the rest of that paper and what else I learned from Ronan who is also by the way the author of the tiny stories paper but back to Gemini if you remember the planning bit from deepmind's earlier systems that reminded me of something else from Gemini's introduction Gemini was created from the ground up to be multi-modal highly efficient a tool and API Integrations and built to enable future Innovations",
        "start": "00:02:00",
        "duration": 236.39999999999998,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "like memory and planning this is echoed in the article in which hasabis says his team will combine a language model like gpt4 with techniques used in alphago aiming to give the system new capabilities such as planning or the ability to solve problems interestingly this comes just a few weeks after deepmind's Extreme risks paper which identified long Horizon planning as a dangerous capability for example adapting its plans in the light of unexpected obstacles or adversaries and generalizing to novel or new setting for me this is a bit like when a model can predict what humans would do in reaction to its own outputs back to the article it's interesting though that asabis is both tasked with accelerating Google's AI efforts while also managing unknown and potentially grave risks so what's his take asaba says the extraordinary potential benefits of AI such as for scientific discovery in areas like health or climate make it imperative that Humanity does not stop developing the tech technology he also believes that mandating a pause is Impractical as it would be near impossible to enforce if done correctly it will be the most beneficial technology for Humanity ever he says of AI we've got to boldly and bravely go after those things so how would alphago become alphago GPT asabis describe the basic approach behind alphago in two of his recent talks so what what's going on here then well effectively if one thinks of a go tree as the tree of all possibilities and imagine each node in this tree is a go position so what we're basically doing is guiding the search with the model so the model is coming up with most probable moves and therefore guiding the tree search to be very efficient and then when it runs out of time of course then it outputs the best tree that is found up to that point we've learned that from data or from simulated data ideally you have both in many cases so",
        "start": "00:03:58",
        "duration": 233.21900000000005,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "in games obviously we have this it's effectively simulated data and then what you do is you take the model and then you use that model to guide a search process According to some objective function I think this is a general way to think about a lot of problems I'm not saying every problem can fit into that I mean maybe and I'll give you example from drug discovery which is what we're trying to do at isomorphic so this is the tree I showed you earlier finding the best go move right and you're trying to find a near optimal or close to Optimal uh go move and go strategy well what happens if we just change those nodes to chemical compounds now let me know in the comments if that reminded anyone else of the truth of thoughts paper in which multiple plans are sampled and results were exponentially better on tasks that GT4 finds impossible like creating workable crossword or mathematical problems that require a bit of planning like creating the greatest integer from a set of four integers using operations like multiplying and addition well I think my theory might have some legs because look at where many of the authors of this paper work and just yesterday as I was researching for this video the tree of thoughts paper was also cited in this paper on using language models to prove mathematical theorems as you can see at the moment gpt4 doesn't do a great job but my point in bringing this up was this they say towards the end of the paper that another key limitation of Chaturbate was its inability to search systematically in a large space remember that's what alphago is really good at we frequently found that it stuck to an unpromising path when the correct solution could be found by backtracking Allah tree of thoughts and exploring alternative paths this behavior is consistent with the general observation that llms are weak at search and planning addressing this weakness is an",
        "start": "00:05:55",
        "duration": 217.98099999999997,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "active area of research and then they reference the tree of thoughts paper it could well be that Gemini let alone Gemini 2 which is state of the art for mathematical theorem proving and to be honest once we can prove theorems we won't be as far from generating new ones and in my opinion fusing this alphago style branching mechanism with a large language model could work for other things we've all seen models like gpt4 sometimes give a bad initial answer picking just the most probable output in a way that's sometimes called 3D decoding but methods like smart GPT and self-consistency demonstrate that the first initial or most probable output doesn't always reflect the best that a model can do and this is just one of the reasons as I said to Ronan I honestly think we could see a model hit 100 in the mmlu in less than five years the mmlu which I talked about in my smart GPT video is a famous machine learning Benchmark testing everything from formal logic to physics and politics and I know that predicting 100 performance within five years is a very bold prediction but that is my prediction but if those are the growing capabilities what does demisasabas think about the implications of the sheer power of such a model one of the biggest challenges right now hasaba says is to determine what the risks of a more capable AI are likely to be I think more research why the field needs to be done very urgently on things like evaluation tests he says to determine how capable and controllable new AI models are he later mentions giving Academia Early Access to these Frontier models and they do seem to be following through on this with deepmind open Ai and anthropic giving Early Access to their Foundation models to the UK AI task force this Foundation model task force is led by Ian Hogarth who was actually the author of this the we must slow down the race to Godlike AI paper that I did a video on back in April do",
        "start": "00:07:44",
        "duration": 235.85900000000004,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "check that video out but in the article Hogarth mentioned a practical plan to transform these companies into a cern-like organization and somewhat unexpectedly this idea was echoed this week by none other than Satya Nadella who had earlier called on Google to quote dance essentially the biggest unsolved problem is how do you ensure both at sort of a scientific understanding level and then the Practical engineering level that you can make sure that the AI never goes out of control and that's where I think there needs to be a CERN like project where both the academics along with corporations and governments all come together to perhaps solve that alignment problem and accelerate the solution to the alignment problem but back to the article the interview with asabes ended with this somewhat chilling response to the question how worried should you be asaba says that no one really knows for sure that AI will become a major danger but he is certain if progress continues at its current Pace there isn't much time to develop safeguards I can see the kind of things we're building into the Gemini series and we have no reason to believe that they won't work my own thoughts on this article are twofold first that we might not want to underestimate Google and hasabis and the adding alphago type systems probably will work and second based on his comments I do think there needs to be more clarity on just how much of Google deepmind's Workforce is working on these evaluations and preemptive measures this article from a few months ago estimates that there may be less than 100 researchers focused on those areas out of thousands so is it even five percent of the total and if not how can we take too seriously the commitments at any AI Summit such as the one happening this autumn in the UK on safety on the other hand if asabis revealed that half or more of his Workforce were on the case",
        "start": "00:09:42",
        "duration": 230.341,
        "title": "Google Gemini: AlphaGo-GPT?"
    },
    {
        "text": "then we could be more confident that the creators of alphago and my fellow londoners had a good chance of researching to safety and success as always thank you so much for watching and have a wonderful day",
        "start": "00:11:37",
        "duration": 28.599999999999998,
        "title": "Google Gemini: AlphaGo-GPT?"
    }
]