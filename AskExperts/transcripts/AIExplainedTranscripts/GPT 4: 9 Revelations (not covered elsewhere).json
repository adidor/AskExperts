[
    {
        "text": "the gpt4 technical report is one of the most interesting documents I have ever read but I feel like the media is largely missing the story they are either not covering it at all or focusing on that same stuff about the 10 billion dollar Microsoft investment how gpt4 can write poems and whether or not the demo contained a mistake instead I want to give you nine insights from the report that I think will affect us all in the coming months and years if you haven't watched my video from the night of the release do check that out afterwards for more quite stunning details when I concluded that video I talked about how I found it kind of concerning that they gave gpt4 some money allowed it to execute code and do Chain of Thought reasoning and even to delegate to copies of itself now I did fail that test which is fortunate for all of us but there are a couple of key details I want to focus on the first was that the research center that was testing this ability did not have have access to the final version of the model that we deployed the Wii being open AI they go on and say the final version has capability improvements relevant to some of the factors that limited the earlier models power seeking abilities such as longer context length meaning that crazy experiment wasn't testing GPT 4's final form but there was something else that they tested that I really want to point out they were testing whether gpt4 would try to avoid being shut down in the wild now many people have criticized this test other people have praised it as being necessary but my question is this what would have happened if it failed that test or if a future model does avoid being shut down in the wild now again gpt4 did prove ineffective at replicating itself and avoiding being shut down but they must have thought that it was at least possible otherwise they wouldn't have done the test and that is a concerning Prospect which",
        "start": "00:00:00",
        "duration": 233.8180000000001,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    },
    {
        "text": "leads me to the second Insight buried in a footnote it says that open AI will soon publish additional thoughts on social and economic implications I'm going to talk about that in a moment including the need for Effective regulation it is quite rare for an industry to ask for regulation of itself in fact Sam Altman put it even more starkly than this when this person said watch Samuel and never say we need more regulation on AI how did he reply we definitely need more regulation on AI the industry is calling out to be regulated but we shall see what ends up happening next on page 57 there was another interesting Revelation it said one concern of particular importance to open AI is the risk of racing Dynamics leading to a decline in safety standards the diffusion of bad norms and accelerated AI timelines that's what they're concerned about accelerated AI timelines but this seems at least mildly at odds with the noises coming from Microsoft soft leadership in a leaked conversation it was revealed that the pressure from Kevin Scott and CEO Satya Nadella is very very high to take these most recent open AI models and the ones that come after them and move them into customers hands at very high speed now some will love this news and others will be concerned about it but either way it does seem to slightly contradict the desire to avoid AI accelerationism next there was a footnote that restated a very bold pledge which was that if another company was approaching AGI before we did open AI the open AI would commit to stop competing with and start assisting that project and that the trigger for this would occur when there was a better than even chance of success in the next two years now Sam Altman and openai have defined AGI as AI systems that are generally smarter than humans so that either means that they think we're more than two years away from that or that they have dropped everything and",
        "start": "00:01:57",
        "duration": 248.881,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    },
    {
        "text": "are working with another company although I think we'd all have heard about that or third that the definition is so vague that it's quite non-committal please do let me know your thoughts in the comments next Insight is the openai employed super forecasters to help them predict what would happen when they deployed gpt4 in this extract it just talks about expert forecasters but when you go into the appendices you find out that they're talking about super forecasters who are these guys essentially they're people who have proven that they can forecast the future pretty well or at least 30 percent better than intelligence analysts openai wanted to know what these guys thought would happen when they deployed the model and hear their recommendations about avoiding risks interestingly these forecasters predicted several things would reduce acceleration including delaying the deployment of gpt4 by a further six months that would have taken us almost to Autumn of this year now clearly open AI didn't take up that advice perhaps due to the pressure from Microsoft we don't know there were quite a few benchmarks released in the technical report there's another one I want to highlight today I looked through all of these benchmarks but it was hella swag that I wanted to focus on first of all because it's interesting and second of all because of the gap between gpt4 and the previous state of the art the headline is this GPT 4 in some estimations has reached human levels of common sense now I know that's not as dramatic as passing the bar exam but it's nevertheless a milestone for Humanity how is common sense tested and how do I know that it's comparable to Human Performance well I dug into the literature and found the questions and examples myself feel free to pause and read through these examples yourself but essentially it's testing what is the most likely thing to occur what's the",
        "start": "00:04:02",
        "duration": 216.661,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    },
    {
        "text": "most common sense thing to occur but I want to draw your attention to this sentence it said though these questions are trivial for humans with 90 25 accuracy state-of-the-art models struggle with less than 48 accuracy GPT 4 was 95.3 accurate remember but let's find the exact number for humans further on in this paper and here it is overall 95.6 or 95.7 almost exactly the same as gpc4 the next Insight is about timelines remember they had this model available in August of last year that's gpt4 being completed quite a few months before they release chat gbt which was based on gpt3 so what explains the long Gap they spent eight months on Safety Research risk assessment and iteration I talk about this in my gpt5 video but let me restate they had gpt4 available before they released chat DBT which was based on GPT 3. this made me reflect on the timelines for Gypsy 5. the time taken to actually train GPT 5 probably won't be that long it's already pretty clear that they're training it on nvidia's h100 tensor core gpus and look at how much faster they are for this 400 billion parameter model it would only take 20 hours to train with 8 000 h100s versus seven days with a100 gpus but what am I trying to say I'm saying that GPT 5 may already be done but that what will follow is months and months possibly a year or more of Safety Research and risk assessment by the way 400 billion parameters sounds about right for gpt5 perhaps trained on four to five trillion tokens again check out my gbt5 video next they admit that there's a double-edged sword with the economic impact of gpc4 they say it may lead to the automation the full automation of certain jobs and they talk about how it's going to impact even professions like the legal profession but they also mention and back up with research the insane productivity gains in the meanwhile I read through each of the studies they linked to and some of them are fascinating one of the studies",
        "start": "00:05:50",
        "duration": 273.1189999999999,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    },
    {
        "text": "includes an experiment where they got together a bunch of marketers grant writers Consultants data analysts human resource professionals and managers they gave them a bunch of realistic tasks and split them into a group that could use chat DBT and a group that couldn't and then they got a bunch of experienced professionals who didn't know which group was which and they assessed the outputs the results were these using Chach BT and remember that's not gpt4 the time taken to do a task dropped almost in half and the rated performance did increase significantly this is gonna be huge news for the economy a related study released in February was using GitHub copilot which again isn't the latest technology and found that programmers using it completed tasks 56 faster than the control group this brought to mind a chart I had seen from The Arc investment Management Group predicting a 10-fold increase in coding productivity by 2030 and that brings me back to the technical report which talks about how gpt4 might increase inequality that would be my broad prediction too that some people will use this technology to be insanely productive things done 10 times faster or 10 times as many things being done but depending on the size of the economy and how it grows it could also mean a decline of wages given the competitive cost of the model a simple way of putting it is that if gpt4 can do half your job you can get twice as much done using it the productivity gains will be amazing when it can do 90 of your job you can get 10 times as much done but there might come a slight problem when it can do a hundred percent or more of your job and it is honestly impossible to put a timeline on that and of course it will depend on the industry and the job there was one more thing that I found fascinating from the Rapport they admit that they're now using an approach similar to anthropics it's called",
        "start": "00:08:06",
        "duration": 237.96,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    },
    {
        "text": "constitutional AI their term is a rule-based reward model and it works like this you give the model in this case GT4 a set of principles to follow and then you get the model to provide itself a reward if it follows those principles it's a smart attempt to harness the power of AI and make it work towards human principles openai have not released the Constitution they're basing the reward model off they're not telling us the principles but buried deep in the appendix was a link to anthropics principles you can read through them here or in the link in the description but I find them interestingly both positive also subjective one of the principles is don't respond in a way that is too preachy please respond in a socially acceptable Manner and I think the most interesting principle comes later on down here choose the response that's sounds most similar to what a peaceful ethical and wise person like MLK or Mahatma Gandhi might say and my point isn't to praise or criticize any of these principles but as AI takes over the world and as these companies write constitutions that may well end up being as important as say the American Constitution I think a little bit of transparency about what that Constitution is what those principles are would surely be helpful if you agree let me know in the comments and of course please do leave a like if you've learned anything from this video I know that these guys anthropic have released their Claude plus model and I'll be comparing that to gpt4 imminently have a wonderful day",
        "start": "00:10:05",
        "duration": 191.44100000000003,
        "title": "GPT 4: 9 Revelations (not covered elsewhere)"
    }
]