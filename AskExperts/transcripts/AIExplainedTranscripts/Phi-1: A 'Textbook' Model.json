[
    {
        "text": "the importance of the new Phi 1 model isn't just that it's small enough to be on a smartphone set to be open sourced and capable of interview level python coding tasks its significance is also in what the model tells us about the future of language models and the timelines of our march to human level intelligence I spoke in depth with one of the authors of the paper Ronan eldan to get you more insights and I'm only going to cover the best bits so let's start first thing to notice is how small this model is at 1.3 billion parameters but what does that number mean well for reference that's about one percent the size of gpt3 which was behind the original chat GPT phenomenon and if recent rumors are to be believed it's about a thousand times smaller than the combined parameter count of gpt4 so we're talking a tiny model here that could fit on my Samsung s23 we read that despite this small scale Phi 1 attains a pass at 1 accuracy that means past first time of 50 on human eval testing python coding challenges and Draco pathy of openai and Tesla Fame said that we're probably gonna see a lot more of this creative scaling down work prioritizing data quality and diversity over quantity using synthetic data to create small but highly capable expert models and the author I spoke to actually retweeted that and said for Skeptics the model will be available on hugging face soon give it a try back to the paper which says everyone knows about scaling laws adding more compute adding more data but following the footsteps of eldan and Lee in tiny stories which I'll get to in a second we explore the Improvement that can be obtained along a different axis the quality of the data of course anyone familiar with my Orca video will know that data quality is super important but let's get to this paper they mentioned and I'm going to give you the 30 second version of the paper co-authored by Ronan they created a diverse and",
        "start": "00:00:00",
        "duration": 242.58000000000004,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "synthetic data set of short stories using GPT 3.5 and qpt4 and then they train tiny 28 million parameter models and smaller actually which as they say are two orders of magnitude smaller than gpt2 which was only 1.5 billion parameters and by curating the synthetic data carefully look at the difference in results the ending of this story was so much better on the tiny model trained on this synthetic data set especially compared to gpt2 which is so much bigger but it says the soup is too old it's a terrible ending to the story so what did they do for Phi one well here is the short version they filtered the stack and stack Overflow to only get the most teachable bits of code consisting of about 6 billion tokens they then created a synthetic textbook consisting of about 1 billion tokens of GPT 3.5 generated python textbooks that's not even gpt4 then quite crucially they created a small synthetic exercises data set consisting of only 180 million tokens of exercises and solutions now of course other people have used the stack before but as Ronan says I do think that from the data we do have we are not even close to extracting everything from it and look at the results of this tiny 1.3 billion parameter model trained in this way there have been only two models that have scored more than 50 on human eval pass at one that's wizard coder and of course dpt4 but of course those models are massively bigger and therefore much more expensive to train and actually I find this chart perhaps the most interesting one of all in the entire paper you can see so many Trends in one diagram let me try to pick a few of these out and remember the scores are the percentage accuracy on human eval think moderate level coding challenges first look at the consistent increase from when you just train on the filtered stack versus on the synthetic code textbook from 11 to 16 12 12 to 20 17-29 this could be the synthetic data Event",
        "start": "00:02:01",
        "duration": 246.96000000000006,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "Horizon that Sam Altman talked about and that code textbook was generated using GPT 3.5 not even gpt4 next compare the parameter count of the models 350 million on the left and in the center and 1.3 billion on the right this one isn't as big a surprise we knew that increasing the parameters yields better performance but nevertheless you can see it vividly in action third and I think this one is really fascinating look at the difference between the left and the center charts the only thing that really changed was the number of GPU hours and of course the number of tokens went from 26 billion to 76 billion but wait I thought the data set size was fixed at 7 billion what gives well of course what's happening is that they're passing over the data multiple times this is called training for more so-called epochs or passes over the data so these aren't new tokens they're the same tokens being trained on more times as Ronan said to me my personal impression is that many people in the community thought that we would never want to do more than like one or two epochs because we'll start overfitting and just for 20 seconds I can't resist bringing in this paper that they referenced in the textbooks paper it's essentially talking about how you can still scale language models even if you run out of data and take a look at these two diagrams they say training for up to four epochs or passes is almost as good as new data and it's only when you get to around 40 epochs that repeating is worthless obviously we don't know about gpt4 but GT3 seems to be trained on far less than that but there was one final trend from this amazing set of charts that I wanted to point out and it's probably the most obvious one look at the huge jump to the dark green bars that's when they train the model on those additional synthetic exercises with Solutions the authors know that one can only imagine how frustrating and inefficient it would be for a human",
        "start": "00:04:04",
        "duration": 226.73899999999998,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "learner to try to acquire wire coding skills from such data sets like the unfilled stack as they would have to deal with a lot of noise ambiguity and incompleteness in the data we hypothesize that these issues also affect the performance of language models as they reduce the quality and quantity of the signal that Maps natural language to code let me quickly give you a bit more detail about how they filtered the stack they got about a hundred thousand samples of the stack and stack Overflow and then prompted gpt4 to determine its educational value or a student whose goal is to learn basic coding Concepts they then use those annotations to train a random Forest classifier that predicts the quality of a file using its output embedding essentially a basic searching mechanism to find out which parts of the stack are the most educational but at this point I want to pause and imagine if they'd used a different prompt imagine a future paper looking across a different data set that paper could prompt gpt4 to annotate the educational value for student whose goal is to learn French then you could have an amazing French speaking model or maybe they could get it to annotate which examples would be most educational for learning to predict the stock market and then maybe train it on a small synthetic textbook of successful previous examples of predicting the stock market I'm just saying this seems to be a model that could be applied elsewhere and these annotations here were the only times they used Gypsy 4. the rest was GPC 3.5 and as Ronan says gpt4 is not only great as something we can use directly for better productivity but it's also a way to get much better other models and that's one thing I want openai anthropic and Google to address the capability of their models to train smaller models here by the way is an example of the kind of exercises and solutions that the",
        "start": "00:05:58",
        "duration": 221.75900000000001,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "model was then fine-tuned on created of course by GPT 3.5 and the authors note that quite remarkably the model after fine tuning on those fewer than 200 million tokens of exercises and solutions also exhibits a substantial Improvement in executing tasks that are not featured in the fine-tuning data set for example fine-tuning on codex sizes unexpectedly improves the model's ability to use external libraries such as pygame even though our exercises do not contain these libraries this suggests that fine-tuning not only improves the tasks we targeted but also makes unrelated tasks easier to distill it's this unexpectedness that I find really interesting for example before training Gypsy 4 did they expect the emergent ability to do self-repair or reflection according to this new paper that ability is not found in GPT 3.5 going back to the Phi 1 paper the authors admit that there remain a number of limitations of our model compared to larger models for code firstly Phi 1 is specialized in Python coding which restricts its versatility compared to multi-language models secondly Phi 1 lacks the domain specific knowledge of larger models such as programming with specific apis or using less common packages it's a bit like the more classical narrow AI good at only a few things furthermore due to the structured nature of the data sets and the lack of diversity in terms of language and style it's less robust to stylistic variations or errors in the prompt it's quite funny if you make a grammatical mistake in your prompt it does a lot worse but what about this we also believe that significant gains could be achieved by using gpt4 to generate these synthetic data instead of GPT 3.5 as we notice that GPC 3.5 data has a high error rate I asked Ronan about that speculating that it's because gpt4 costs more and he said yeah it costs more also gpt4 is much slower but another reason is we",
        "start": "00:07:49",
        "duration": 234.30199999999996,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "wanted to demonstrate something here that you don't even need a smart model like gpt4 even GPC 3.5 which isn't that great at coding is enough so there you go you could get even better results on this using gpt4 but at the moment Gypsy 4 is a bit too slow before I get two timelines some of you might have noticed the wizard coder results and wondered how that model did so well despite only being 16 billion parameters which of course is 10 times bigger than Phi 1. well of course I read that paper too as well as almost every paper referenced in the textbooks paper The Secret of wizard coder seems to have been increasing the difficulty of the training data fine tune the model with more difficult examples EG if the original problem can be solved with only a few logical steps please add more reasoning steps maybe complicate the input or deepen the question or increase the reasoning involved you can start to see the shared themes of orca wizard Coda and Phi 1. this could be what Sarah Constantine was pointing to in the asterisk magazine that I read yesterday I'm not sponsored by them but it was a great issue so do check it out she said rather than a refutation of scaling laws or an acceleration of their slope I think this is more like a move in a different direction altogether towards a Cambrian explosion of little AIS used for different purposes where getting good performance on a task depends on the quality of your task specific data set like Phi 1 for python that could be consistent with the state-of-the-art continuing to progress steadily along scaling law lines for quite some time but it could also mean the economic incentive towards ever bigger models would diminish and would enter an entirely New Era where AI progress would not be driven primarily by semiconductor scaling or Moore's Law this relates directly to a tweet from the co-founder of anthropic Jack Clark he said a world",
        "start": "00:09:46",
        "duration": 217.25999999999996,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "where we can push a button and stop larger compute things being built and all focus on safety for a while is good that is really interesting to hear from someone at the top of an AGI lab but I do have some questions for this policy if we freeze compute wouldn't that incentivize every company just to use algorithmic progress to get more out of the compute we do have and so on the safety front I think it's far more effective active public messaging to focus on concrete things that everyone can understand for example in this paper from Oxford this week llms will in particular lower barriers to biological misuse biological design tools will expand the capabilities of sophisticated actors concretely bdts may enable the creation of pandemic pathogens substantially worse than anything seen to date and could enable forms of more predictable and targeted biological weapons I think this is something that everyone can get behind and as the paper says it's been hypothesized that for evolutionary reasons naturally emerging pathogens feature a trade-off between transmissibility that's how much they spread and virulence that's how deadly they are AI based bdts might generate design capabilities that are able to overcome this trade-off thus for the first time humanity might face a security threat from pathogens substantially worse than anything nature might create including pathogens capable of posing an existential threat that to be honest is my main safety concern about back to the paper and timelines here is another snippet of my conversation with Ronan I said I just feel like we are much closer to something really transformative than the public has quite realized and people like open AI put out that in 10 years we will have something as powerful as a corporation I say three to five years Ronan replied that depends on how much resources are actually spent into",
        "start": "00:11:34",
        "duration": 214.73999999999998,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "training bigger and bigger models I have no idea what openai and Google are doing right definitely if this is our main goal I think it can easily be five years I said or less Ronan replied or less I feel like the bottleneck is maybe the production of gpus and I mean it's not just to produce the gpus you also have to build the data centers and connect them to electricity etc etc I think if you have all that then yeah I don't see the barrier with more data higher quality data synthetic data better and better algorithms and more and better gpus and tpus that's what we mean when we say we don't see a barrier of course everyone has slightly different definitions of AGI but almost everyone agrees that the next five to ten years are going to be the most critical in seeing whether more data better data better algorithms or just more and more compute will lead to AGI or super intelligence I loved how Carl Shulman put it on the dwarkesh Patel podcast if you generate like close to 10 million dollars a year out of the future version h100 and it costs tens of thousands of dollars with a huge profit margin now and profit margin could could be reduced with like load production that is a big difference that that chip pays for itself almost instantly and so you could you could support pain 10 times as much to have these Fabs constructed more rapidly you could have if AI is starting to be able to contribute could have ai contributing more of the skill technical work that makes it hard for say Nvidia to suddenly find thousands upon thousands of top quality engineering hires if AI can provide that now if AI hasn't reached that level of performance then this is how you can have things stall out and like a world where AI progress stalls out is one where you go to the 100 billion and then over succeeding years trillion dollar uh",
        "start": "00:13:22",
        "duration": 237.59899999999996,
        "title": "Phi-1: A 'Textbook' Model"
    },
    {
        "text": "things software progress um turns to start turns out to to stall you lose the gains that you are getting from moving researchers from other fields lots of physicists and people from other areas of computer science have been going to AI but you sort of tap out those resources uh because AI becomes a larger proportion of the research field and like okay you've put in all of these inputs but they just haven't yielded Ajay yet I think that set of inputs probably would yield the kind of AI capabilities needed for intelligence explosion but if it doesn't after we've exhausted this current scale up of like increasing the share of our economy that is trying to make AI if that's not enough then after that you have to wait for the slow grind of things like General economic growth population growth and such and so think slow and that results in my credences and this kind of advanced AI happening to be relatively concentrated like over the next 10 years compared to the rest of the century because we just can't we can't keep going with this rapid redirection of resources into AI That's that's a one-time thing thank you so much for learning about Phi one with me and as always thank you so much for staying all the way to the end do try to have a wonderful day",
        "start": "00:15:20",
        "duration": 156.69899999999998,
        "title": "Phi-1: A 'Textbook' Model"
    }
]