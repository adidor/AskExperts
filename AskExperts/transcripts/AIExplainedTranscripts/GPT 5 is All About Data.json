[
    {
        "text": "find out what I could about gpt5 I have read every academic paper I could find about it every leak report interview snippet and media article I can summarize it like this it will come down to data how much of it there is how it's used and where it comes from these are the factors that will dictate whether GPT 5 gets released later this year and whether it will actually approach genius level IQ some media reports have picked up on this potential leak about gpt5 you can read it here I have put quite a few hours in trying to verify whether this might be accurate and even though it's now being quoted by reputable sources I still can't confirm its accuracy so for now I'll just say that the rest of the document seems accurate but who knows I am not relying on this for my research about gpt5 but the scale 25 000 gpus does seem right tech radar here describes Chachi BT as having been trained on 10 1000 Nvidia gpus and don't forget those were a 100 gpus Microsoft might well now have access to the h100 GPU which according to every source is a big step up from a100 gpus on pretty much every metric and what about timelines for GPT 5 would later this year be accurate well we can infer from Geordie rybass that gpt4 or equivalent was completed sometime around late spring early summer of 2022 that would be just around the time that deepmind published this which in massively oversimplified terms lays out a framework for optimizing parameter size with the number of training tokens AKA how much info from the web it's trained on turns out models like gpt3 and palm had way more parameters than needed anyway it was the data and especially high quality data that it was lacking so all those laughs about gpt4 needing a hundred trillion parameters were absolutely farcical it could even be that gpt5 has the same or fewer parameters than gpt4 this less wrong post from July of 2022 picks up on that",
        "start": "00:00:00",
        "duration": 272.9409999999999,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "finding and points out that it is Data not size that is currently the active constraint on language modeling performance current returns to additional data are immense and current returns to additional model size are minuscule indeed most recent Landmark models are wastefully big if we can leverage enough data there is no reason to run 500 billion parameter models much less 1 trillion parameter or larger models remember it's data not parameter count the link to all of these articles by the way will be in the description at this point let me quickly say that if you're learning anything don't forget to leave a like or a comment frankly even abuse helps the algorithm so go for it what about chat GPT while gpt3 along with a host of other models was trained on about 300 billion tokens by the way what defines a token shifts in the literature but it's somewhere between 1 and 1.4 words therefore think of a token as roughly one word as you can see from the graph below Palm was trained on about 800 billion tokens approximately deepmind's chinchilla on about 1.4 trillion tokens that particular less wrong post was referenced here in this academic paper released in October this paper is absolutely key to this video it's focused entirely on whether we will run out of data as it pertains to machine learning and large language models one of the key takeaways of this paper is the approximation given the how much high quality data slash tokens might be out there the stock of high quality language data is approximated at between 4.6 trillion and 17 trillion words the next point it makes is key we are within one order of magnitude of exhausting high quality data and this will likely happen between 2023 and 2027. for those that don't know being an order of magnitude bigger means being 10 times bigger than what came previously now I want you to remember that 2023 to 27 timeline for a moment because first I",
        "start": "00:02:17",
        "duration": 256.559,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "want to mention why high quality data is important running out of that could mean running out of the rapid improvements in GPT models the paper says models trained on the latter kind of high quality data perform better so it is common practice to use high quality data for training language models and where does that high quality data come from well to be honest not knowing that is a big part of the problem which we will definitely come back to but here is a rough idea we have scientific papers books scraped content from the the web the news code Etc plus Wikipedia of course the paper also mentions here the middle of the road estimate of nine trillion tokens of high quality data available that estimate will be Central in defining the near-term future of artificial intelligence one order of magnitude more as an increase in performance is a huge deal that would change everything but I must say this estimate contrasts with some others such as the 3.2 trillion token estimate from that original post and the author did say that they were trying to make it an overestimate and what about this from David Chapman a PhD in AI from MIT he references the deepmind study and that less wrong post and makes two important and plausible observations first that gpt4 or Bing may have scraped the bottom of the web text barrel and that this might be why it's responses sometimes turn out like emoting teenagers I actually did a video on the crazy conversations you can have with Bing that you can check out after this one but second He suggests that there might be a reason that neither Google nor open AI have been forthcoming about where they get their data from now I'm not saying it might be about illegality but it might be about avoiding controversy over attribution and compensation take me I have math tutorials on the web that I'm sure have been scraped and now lo and behold Bing can teach math I'm not complaining but",
        "start": "00:04:25",
        "duration": 260.581,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "it would be nice to at least know what has been used and what hasn't this of course mirrors the Raging legal issues around AI image generation fights that are only just beginning for these web tags wanting to know where the data came from is going to become a huge issue and this article lays out just some of the surprising sources of data for Google's bad model check out one of them which is YouTube could it be that your comments right now are being harvested quite possibly I want to get back to the central question what are the gpt5 well here on the far right is Google Palms performance which if you remember back from the earlier paper was powered by only 800 billion tokens and palm was definitely not optimized for parameters GPT 5 will learn the lessons from this and will probably scrape as much high quality data as it possibly can and don't forget another year has gone by since gpt4 was handed to Microsoft and the stock of high quality data Grows by around 10 annually anyway even without further efficiencies in data use or extraction so even if Bing did use all the high quality data available I don't think it did and even if David Chapman is right the stock of data now available is going to be greater but if Bing was trained on a similar amount of data to Palm say one trillion tokens but now GPT 5 maxes out we could genuinely be talking about an order of magnitude Improvement I'm going to briefly survey some of the implications of that in a moment before I do I want to show you the ways the openai will likely be improving GPT 5 regardless of previous limitations first more ways might be found to extract high quality data from low quality sources no offense Facebook second this paper from only last week shows that gains can be made by automating Chain of Thought prompting into the model if you're not sure what Chain of Thought prompting is it's a form of prompt engineering that I",
        "start": "00:06:36",
        "duration": 260.5799999999999,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "discussed in my video eight upgrades in gpt4 where essentially you force the model to lay out it's working and thereby improve its output now this paper talks about two to three percent gains but even though small gains when Bing is already this strong would be significant don't forget these are separate upgrades to the data discussion third this paper from three weeks ago shows that language models can teach themselves to use tools such as calculators calendars and apis if there were no other improvements honestly in GPT 5 other than this it would change the world and I know for a fact that people are working on integrating Wolfram Alpha into a large language model and look at the number of tools that Wolfram Alpha has in science math money and more these models can actually teach themselves how to use tools and that Chimes perfectly with this paper which essentially lays out that using a python interpreter models can actually check if their code compiles and thereby teach themselves better coding the links to all of these papers will be in the description as I said the fourth way that GPT 5 might be improved even without more high quality data would be it being trained multiple times on the same data as laid out here by Professor swayam dipta he says that currently these models are trained on the same data just once owing to Performance and cost constraints but it may be possible to train a model several times using the same data sure it might cost more but I think that for Microsoft when all of search and its profits is the prize a few billion could be deemed worth it and this paper co-authored by that same Professor lays out how models can generate additional data sets on problems with which they struggle such as those with complex pans and that humans could filter their answers for correctness think of this as artificial data generation and it can lead to 10 or",
        "start": "00:08:46",
        "duration": 249.659,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "more in improvements and if artificial data can be integrated honestly what is actually going to bottleneck these GPT models I could go on with the improvements that might be made without new data my central point is that data will be the big determinant but there are other ways to improve gpd5 if data turns out to be a bottleneck what if they can fully utilize 9 trillion tokens as the original paper surmised by the end of 2024 or even the beginning of 2024 what could one more order of magnitude Improvement actually look like the short answer is that no one knows probably not AGI but certainly a revolution in the jobs Market maybe this is why Sam Altman tweeted 2023 thirty thousand dollars to get a simple iPhone app created 300 for a plumbing job I wonder what those relative prices will look like in 2028 the likely coming Divergence between changes to cognitive work and changes to physical work could be quite dramatic that gives a sense of his timelines but my own guess is that the best human raters will be beaten on at least some of the following benchmarks take reading comprehension where you can imagine the extrapolation to gpt5 if and when it occurs that would have huge implications for summarization and creative writing next logic and critical reasoning we're talking debating topics doing law work Discerning causality in complex scenarios that would be huge in finance where you have to sort the signal from the noise in large data sets physics and high school math would be close to solved by an order of magnitude Improvement AI tutors replacing my job for example could be with us by the end of next year don't forget the release of GPT 5 in whichever month it comes will likely roughly coincide with the final refinements in text to speech image to text text to image and text to video avatars so don't think AI tutors are as far as you might imagine the reason why",
        "start": "00:10:51",
        "duration": 253.92000000000004,
        "title": "GPT 5 is All About Data"
    },
    {
        "text": "no one on and certainly not me can be sure of timelines for GT5 though it's because they depend partly on internal Safety Research at Google and openai take this quote from Sam Altman to the New York Times and when we are ready when we think we have completed our alignment work and all of our safety thinking and worked with external Auditors other AGI Labs then will release those things here he's probably talking about gpt4 but the same would apply even more so to gbt5 on the other hand the release and then on release of the Sydney model of Bing might suggest otherwise but at least according to him safety and Alignment are the goal I'm going to end with this quote from samuelman again he added the blue text last minute to his public post on AGI released the other week it says it's important that the ratio of safety progress to capability progress increases in other in other words these models are getting much more powerful much faster than they can keep up with but thank you for keeping up with this video thank you for watching to the end please do check out my other videos on Bing chat and its use cases and either way have a wonderful day",
        "start": "00:12:57",
        "duration": 158.26099999999997,
        "title": "GPT 5 is All About Data"
    }
]